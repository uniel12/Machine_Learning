{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beee65be",
   "metadata": {},
   "source": [
    "### 단어 토큰화(Word Tolenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58a151",
   "metadata": {},
   "source": [
    "- 코퍼스(Corpus) : 분석에 활용하기 위한 자연어 데이터\n",
    "- 토큰화: 하나의 코퍼스를 여러 개의 토큰(가장 작은 의미의 단위)으로 나누는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3814a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 자연어 처리 패키지 : NLTK\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6154a2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smhrd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# punkt(토큰화 모듈) : 마침표나 약어(Mr,Dr) 고려하여 진행\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b60370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'it', \"'s\", 'not', 'a', 'happily-ever-after', 'ending', ',', 'it', 'is', 'very', 'realistic', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Although it's not a happily-ever-after ending, it is very realistic.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = word_tokenize(text)\n",
    "\n",
    "# 띄어쓰기, 어퍼스트로피('),콤마(,)가 기준이 됨 \n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f106b88a",
   "metadata": {},
   "source": [
    "### 실습\n",
    "- text.py 파일에는 영어 자연어 코퍼스(말뭉치)가 있음. 해당 코퍼스를 불러와서 단어 토큰화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "151a2a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'reading', 'the', 'comments', 'for', 'this', 'movie', ',', 'I', 'am', 'not', 'sure', 'whether', 'I', 'should', 'be', 'angry', ',', 'sad', 'or', 'sickened', '.', 'Seeing', 'comments', 'typical', 'of', 'people', 'who', 'a', ')', 'know', 'absolutely', 'nothing', 'about', 'the', 'military', 'or', 'b', ')', 'who', 'base', 'everything', 'they', 'think', 'they', 'know', 'on', 'movies', 'like', 'this', 'or', 'on', 'CNN', 'reports', 'about', 'Abu-Gharib', 'makes', 'me', 'wonder', 'about', 'the', 'state', 'of', 'intellectual', 'stimulation', 'in', 'the', 'world', '.', 'At', 'the', 'time', 'I', 'type', 'this', 'the', 'number', 'of', 'people', 'in', 'the', 'US', 'military', ':', '1.4', 'million', 'on', 'Active', 'Duty', 'with', 'another', 'almost', '900,000', 'in', 'the', 'Guard', 'and', 'Reserves', 'for', 'a', 'total', 'of', 'roughly', '2.3', 'million', '.', 'The', 'number', 'of', 'people', 'indicted', 'for', 'abuses', 'at', 'at', 'Abu-Gharib', ':', 'Currently', 'less', 'than', '20', 'That', 'makes', 'the', 'total', 'of', 'people', 'indicted', '.00083', '%', 'of', 'the', 'total', 'military', '.', 'Even', 'if', 'you', 'indict', 'every', 'single', 'military', 'member', 'that', 'ever', 'stepped', 'in', 'to', 'Abu-Gharib', ',', 'you', 'would', 'not', 'come', 'close', 'to', 'making', 'that', 'a', 'whole', 'number', '.', 'The', 'flaws', 'in', 'this', 'movie', 'would', 'take', 'YEARS', 'to', 'cover', '.', 'I', 'understand', 'that', 'it', \"'s\", 'supposed', 'to', 'be', 'sarcastic', ',', 'but', 'in', 'reality', ',', 'the', 'writer', 'and', 'director', 'are', 'trying', 'to', 'make', 'commentary', 'about', 'the', 'state', 'of', 'the', 'military', 'without', 'an', 'enemy', 'to', 'fight', '.', 'In', 'reality', ',', 'the', 'US', 'military', 'has', 'been', 'at', 'its', 'busiest', 'when', 'there', 'are', 'not', 'conflicts', 'going', 'on', '.', 'The', 'military', 'is', 'the', 'first', 'called', 'for', 'disaster', 'relief', 'and', 'humanitarian', 'aid', 'missions', '.', 'When', 'the', 'tsunami', 'hit', 'Indonesia', ',', 'devestating', 'the', 'region', ',', 'the', 'US', 'military', 'was', 'the', 'first', 'on', 'the', 'scene', '.', 'When', 'the', 'chaos', 'of', 'the', 'situation', 'overwhelmed', 'the', 'local', 'governments', ',', 'it', 'was', 'military', 'leadership', 'who', 'looked', 'at', 'their', 'people', ',', 'the', 'same', 'people', 'this', 'movie', 'mocks', ',', 'and', 'said', 'make', 'it', 'happen', '.', 'Within', 'hours', ',', 'food', 'aid', 'was', 'reaching', 'isolated', 'villages', '.', 'Within', 'days', ',', 'airfields', 'were', 'built', ',', 'cargo', 'aircraft', 'started', 'landing', 'and', 'a', 'food', 'distribution', 'system', 'was', 'up', 'and', 'running', '.', 'Hours', 'and', 'days', ',', 'not', 'weeks', 'and', 'months', '.', 'Yes', 'there', 'are', 'unscrupulous', 'people', 'in', 'the', 'US', 'military', '.', 'But', 'then', ',', 'there', 'are', 'in', 'every', 'walk', 'of', 'life', ',', 'every', 'occupation', '.', 'But', 'to', 'see', 'people', 'on', 'this', 'website', 'decide', 'that', '2.3', 'million', 'men', 'and', 'women', 'are', 'all', 'criminal', ',', 'with', 'nothing', 'on', 'their', 'minds', 'but', 'thoughts', 'of', 'destruction', 'or', 'mayhem', 'is', 'an', 'absolute', 'disservice', 'to', 'the', 'things', 'that', 'they', 'do', 'every', 'day', '.', 'One', 'person', 'on', 'this', 'website', 'even', 'went', 'so', 'far', 'as', 'to', 'say', 'that', 'military', 'members', 'are', 'in', 'it', 'for', 'personal', 'gain', '.', 'Wow', '!', 'Entry', 'level', 'personnel', 'make', 'just', 'under', '$', '8.00', 'an', 'hour', 'assuming', 'a', '40', 'hour', 'work', 'week', '.', 'Of', 'course', ',', 'many', 'work', 'much', 'more', 'than', '40', 'hours', 'a', 'week', 'and', 'those', 'in', 'harm', \"'s\", 'way', 'typically', 'put', 'in', '16-18', 'hour', 'days', 'for', 'months', 'on', 'end', '.', 'That', 'makes', 'the', 'pay', 'well', 'under', 'minimum', 'wage', '.', 'So', 'much', 'for', 'personal', 'gain', '.', 'I', 'beg', 'you', ',', 'please', 'make', 'yourself', 'familiar', 'with', 'the', 'world', 'around', 'you', '.', 'Go', 'to', 'a', 'nearby', 'base', ',', 'get', 'a', 'visitor', 'pass', 'and', 'meet', 'some', 'of', 'the', 'men', 'and', 'women', 'you', 'are', 'so', 'quick', 'to', 'disparage', '.', 'You', 'would', 'be', 'surprised', '.', 'The', 'military', 'no', 'longer', 'accepts', 'people', 'in', 'lieu', 'of', 'prison', 'time', '.', 'They', 'require', 'a', 'minimum', 'of', 'a', 'GED', 'and', 'prefer', 'a', 'high', 'school', 'diploma', '.', 'The', 'middle', 'ranks', 'are', 'expected', 'to', 'get', 'a', 'minimum', 'of', 'undergraduate', 'degrees', 'and', 'the', 'upper', 'ranks', 'are', 'encouraged', 'to', 'get', 'advanced', 'degrees', '.']\n"
     ]
    }
   ],
   "source": [
    "from text import TEXT\n",
    "corpus = TEXT\n",
    "tokenized_words = word_tokenize(corpus)\n",
    "print(tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528613d",
   "metadata": {},
   "source": [
    "### 정제(Cleaning)\n",
    "- 코퍼스에는 아무 의미도 없거나 목적에 부합하지 않은 단어들도 포함\n",
    "- 전처리 과정에서 토큰들을 제거하는 작업을 **정제**\n",
    "- 등장빈도, 단어길이, 불용어 등을 기준으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88463528",
   "metadata": {},
   "source": [
    "1. 등장빈도가 적은 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "856626f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장빈도가 2이하인 단어들만 추출\n",
    "\n",
    "# 단어의 빈도 계산에 사용할 Counter() 함수 사용\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e5accb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'After': 1,\n",
       "         'reading': 1,\n",
       "         'the': 30,\n",
       "         'comments': 2,\n",
       "         'for': 7,\n",
       "         'this': 7,\n",
       "         'movie': 3,\n",
       "         ',': 21,\n",
       "         'I': 5,\n",
       "         'am': 1,\n",
       "         'not': 4,\n",
       "         'sure': 1,\n",
       "         'whether': 1,\n",
       "         'should': 1,\n",
       "         'be': 3,\n",
       "         'angry': 1,\n",
       "         'sad': 1,\n",
       "         'or': 4,\n",
       "         'sickened': 1,\n",
       "         '.': 28,\n",
       "         'Seeing': 1,\n",
       "         'typical': 1,\n",
       "         'of': 15,\n",
       "         'people': 9,\n",
       "         'who': 3,\n",
       "         'a': 12,\n",
       "         ')': 2,\n",
       "         'know': 2,\n",
       "         'absolutely': 1,\n",
       "         'nothing': 2,\n",
       "         'about': 4,\n",
       "         'military': 12,\n",
       "         'b': 1,\n",
       "         'base': 2,\n",
       "         'everything': 1,\n",
       "         'they': 3,\n",
       "         'think': 1,\n",
       "         'on': 9,\n",
       "         'movies': 1,\n",
       "         'like': 1,\n",
       "         'CNN': 1,\n",
       "         'reports': 1,\n",
       "         'Abu-Gharib': 3,\n",
       "         'makes': 3,\n",
       "         'me': 1,\n",
       "         'wonder': 1,\n",
       "         'state': 2,\n",
       "         'intellectual': 1,\n",
       "         'stimulation': 1,\n",
       "         'in': 12,\n",
       "         'world': 2,\n",
       "         'At': 1,\n",
       "         'time': 2,\n",
       "         'type': 1,\n",
       "         'number': 3,\n",
       "         'US': 4,\n",
       "         ':': 2,\n",
       "         '1.4': 1,\n",
       "         'million': 3,\n",
       "         'Active': 1,\n",
       "         'Duty': 1,\n",
       "         'with': 3,\n",
       "         'another': 1,\n",
       "         'almost': 1,\n",
       "         '900,000': 1,\n",
       "         'Guard': 1,\n",
       "         'and': 14,\n",
       "         'Reserves': 1,\n",
       "         'total': 3,\n",
       "         'roughly': 1,\n",
       "         '2.3': 2,\n",
       "         'The': 5,\n",
       "         'indicted': 2,\n",
       "         'abuses': 1,\n",
       "         'at': 4,\n",
       "         'Currently': 1,\n",
       "         'less': 1,\n",
       "         'than': 2,\n",
       "         '20': 1,\n",
       "         'That': 2,\n",
       "         '.00083': 1,\n",
       "         '%': 1,\n",
       "         'Even': 1,\n",
       "         'if': 1,\n",
       "         'you': 5,\n",
       "         'indict': 1,\n",
       "         'every': 4,\n",
       "         'single': 1,\n",
       "         'member': 1,\n",
       "         'that': 6,\n",
       "         'ever': 1,\n",
       "         'stepped': 1,\n",
       "         'to': 13,\n",
       "         'would': 3,\n",
       "         'come': 1,\n",
       "         'close': 1,\n",
       "         'making': 1,\n",
       "         'whole': 1,\n",
       "         'flaws': 1,\n",
       "         'take': 1,\n",
       "         'YEARS': 1,\n",
       "         'cover': 1,\n",
       "         'understand': 1,\n",
       "         'it': 4,\n",
       "         \"'s\": 2,\n",
       "         'supposed': 1,\n",
       "         'sarcastic': 1,\n",
       "         'but': 2,\n",
       "         'reality': 2,\n",
       "         'writer': 1,\n",
       "         'director': 1,\n",
       "         'are': 9,\n",
       "         'trying': 1,\n",
       "         'make': 4,\n",
       "         'commentary': 1,\n",
       "         'without': 1,\n",
       "         'an': 3,\n",
       "         'enemy': 1,\n",
       "         'fight': 1,\n",
       "         'In': 1,\n",
       "         'has': 1,\n",
       "         'been': 1,\n",
       "         'its': 1,\n",
       "         'busiest': 1,\n",
       "         'when': 1,\n",
       "         'there': 3,\n",
       "         'conflicts': 1,\n",
       "         'going': 1,\n",
       "         'is': 2,\n",
       "         'first': 2,\n",
       "         'called': 1,\n",
       "         'disaster': 1,\n",
       "         'relief': 1,\n",
       "         'humanitarian': 1,\n",
       "         'aid': 2,\n",
       "         'missions': 1,\n",
       "         'When': 2,\n",
       "         'tsunami': 1,\n",
       "         'hit': 1,\n",
       "         'Indonesia': 1,\n",
       "         'devestating': 1,\n",
       "         'region': 1,\n",
       "         'was': 4,\n",
       "         'scene': 1,\n",
       "         'chaos': 1,\n",
       "         'situation': 1,\n",
       "         'overwhelmed': 1,\n",
       "         'local': 1,\n",
       "         'governments': 1,\n",
       "         'leadership': 1,\n",
       "         'looked': 1,\n",
       "         'their': 2,\n",
       "         'same': 1,\n",
       "         'mocks': 1,\n",
       "         'said': 1,\n",
       "         'happen': 1,\n",
       "         'Within': 2,\n",
       "         'hours': 2,\n",
       "         'food': 2,\n",
       "         'reaching': 1,\n",
       "         'isolated': 1,\n",
       "         'villages': 1,\n",
       "         'days': 3,\n",
       "         'airfields': 1,\n",
       "         'were': 1,\n",
       "         'built': 1,\n",
       "         'cargo': 1,\n",
       "         'aircraft': 1,\n",
       "         'started': 1,\n",
       "         'landing': 1,\n",
       "         'distribution': 1,\n",
       "         'system': 1,\n",
       "         'up': 1,\n",
       "         'running': 1,\n",
       "         'Hours': 1,\n",
       "         'weeks': 1,\n",
       "         'months': 2,\n",
       "         'Yes': 1,\n",
       "         'unscrupulous': 1,\n",
       "         'But': 2,\n",
       "         'then': 1,\n",
       "         'walk': 1,\n",
       "         'life': 1,\n",
       "         'occupation': 1,\n",
       "         'see': 1,\n",
       "         'website': 2,\n",
       "         'decide': 1,\n",
       "         'men': 2,\n",
       "         'women': 2,\n",
       "         'all': 1,\n",
       "         'criminal': 1,\n",
       "         'minds': 1,\n",
       "         'thoughts': 1,\n",
       "         'destruction': 1,\n",
       "         'mayhem': 1,\n",
       "         'absolute': 1,\n",
       "         'disservice': 1,\n",
       "         'things': 1,\n",
       "         'do': 1,\n",
       "         'day': 1,\n",
       "         'One': 1,\n",
       "         'person': 1,\n",
       "         'even': 1,\n",
       "         'went': 1,\n",
       "         'so': 2,\n",
       "         'far': 1,\n",
       "         'as': 1,\n",
       "         'say': 1,\n",
       "         'members': 1,\n",
       "         'personal': 2,\n",
       "         'gain': 2,\n",
       "         'Wow': 1,\n",
       "         '!': 1,\n",
       "         'Entry': 1,\n",
       "         'level': 1,\n",
       "         'personnel': 1,\n",
       "         'just': 1,\n",
       "         'under': 2,\n",
       "         '$': 1,\n",
       "         '8.00': 1,\n",
       "         'hour': 3,\n",
       "         'assuming': 1,\n",
       "         '40': 2,\n",
       "         'work': 2,\n",
       "         'week': 2,\n",
       "         'Of': 1,\n",
       "         'course': 1,\n",
       "         'many': 1,\n",
       "         'much': 2,\n",
       "         'more': 1,\n",
       "         'those': 1,\n",
       "         'harm': 1,\n",
       "         'way': 1,\n",
       "         'typically': 1,\n",
       "         'put': 1,\n",
       "         '16-18': 1,\n",
       "         'end': 1,\n",
       "         'pay': 1,\n",
       "         'well': 1,\n",
       "         'minimum': 3,\n",
       "         'wage': 1,\n",
       "         'So': 1,\n",
       "         'beg': 1,\n",
       "         'please': 1,\n",
       "         'yourself': 1,\n",
       "         'familiar': 1,\n",
       "         'around': 1,\n",
       "         'Go': 1,\n",
       "         'nearby': 1,\n",
       "         'get': 3,\n",
       "         'visitor': 1,\n",
       "         'pass': 1,\n",
       "         'meet': 1,\n",
       "         'some': 1,\n",
       "         'quick': 1,\n",
       "         'disparage': 1,\n",
       "         'You': 1,\n",
       "         'surprised': 1,\n",
       "         'no': 1,\n",
       "         'longer': 1,\n",
       "         'accepts': 1,\n",
       "         'lieu': 1,\n",
       "         'prison': 1,\n",
       "         'They': 1,\n",
       "         'require': 1,\n",
       "         'GED': 1,\n",
       "         'prefer': 1,\n",
       "         'high': 1,\n",
       "         'school': 1,\n",
       "         'diploma': 1,\n",
       "         'middle': 1,\n",
       "         'ranks': 2,\n",
       "         'expected': 1,\n",
       "         'undergraduate': 1,\n",
       "         'degrees': 2,\n",
       "         'upper': 1,\n",
       "         'encouraged': 1,\n",
       "         'advanced': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 단어 토큰 리스트 = tokenized_words\n",
    "\n",
    "# Counter 함수를 통해 단어의 빈도 수를 카운트하여 집합 생성\n",
    "vocab = Counter(tokenized_words)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1150324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'reading',\n",
       " 'comments',\n",
       " 'am',\n",
       " 'sure',\n",
       " 'whether',\n",
       " 'should',\n",
       " 'angry',\n",
       " 'sad',\n",
       " 'sickened',\n",
       " 'Seeing',\n",
       " 'typical',\n",
       " ')',\n",
       " 'know',\n",
       " 'absolutely',\n",
       " 'nothing',\n",
       " 'b',\n",
       " 'base',\n",
       " 'everything',\n",
       " 'think',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'CNN',\n",
       " 'reports',\n",
       " 'me',\n",
       " 'wonder',\n",
       " 'state',\n",
       " 'intellectual',\n",
       " 'stimulation',\n",
       " 'world',\n",
       " 'At',\n",
       " 'time',\n",
       " 'type',\n",
       " ':',\n",
       " '1.4',\n",
       " 'Active',\n",
       " 'Duty',\n",
       " 'another',\n",
       " 'almost',\n",
       " '900,000',\n",
       " 'Guard',\n",
       " 'Reserves',\n",
       " 'roughly',\n",
       " '2.3',\n",
       " 'indicted',\n",
       " 'abuses',\n",
       " 'Currently',\n",
       " 'less',\n",
       " 'than',\n",
       " '20',\n",
       " 'That',\n",
       " '.00083',\n",
       " '%',\n",
       " 'Even',\n",
       " 'if',\n",
       " 'indict',\n",
       " 'single',\n",
       " 'member',\n",
       " 'ever',\n",
       " 'stepped',\n",
       " 'come',\n",
       " 'close',\n",
       " 'making',\n",
       " 'whole',\n",
       " 'flaws',\n",
       " 'take',\n",
       " 'YEARS',\n",
       " 'cover',\n",
       " 'understand',\n",
       " \"'s\",\n",
       " 'supposed',\n",
       " 'sarcastic',\n",
       " 'but',\n",
       " 'reality',\n",
       " 'writer',\n",
       " 'director',\n",
       " 'trying',\n",
       " 'commentary',\n",
       " 'without',\n",
       " 'enemy',\n",
       " 'fight',\n",
       " 'In',\n",
       " 'has',\n",
       " 'been',\n",
       " 'its',\n",
       " 'busiest',\n",
       " 'when',\n",
       " 'conflicts',\n",
       " 'going',\n",
       " 'is',\n",
       " 'first',\n",
       " 'called',\n",
       " 'disaster',\n",
       " 'relief',\n",
       " 'humanitarian',\n",
       " 'aid',\n",
       " 'missions',\n",
       " 'When',\n",
       " 'tsunami',\n",
       " 'hit',\n",
       " 'Indonesia',\n",
       " 'devestating',\n",
       " 'region',\n",
       " 'scene',\n",
       " 'chaos',\n",
       " 'situation',\n",
       " 'overwhelmed',\n",
       " 'local',\n",
       " 'governments',\n",
       " 'leadership',\n",
       " 'looked',\n",
       " 'their',\n",
       " 'same',\n",
       " 'mocks',\n",
       " 'said',\n",
       " 'happen',\n",
       " 'Within',\n",
       " 'hours',\n",
       " 'food',\n",
       " 'reaching',\n",
       " 'isolated',\n",
       " 'villages',\n",
       " 'airfields',\n",
       " 'were',\n",
       " 'built',\n",
       " 'cargo',\n",
       " 'aircraft',\n",
       " 'started',\n",
       " 'landing',\n",
       " 'distribution',\n",
       " 'system',\n",
       " 'up',\n",
       " 'running',\n",
       " 'Hours',\n",
       " 'weeks',\n",
       " 'months',\n",
       " 'Yes',\n",
       " 'unscrupulous',\n",
       " 'But',\n",
       " 'then',\n",
       " 'walk',\n",
       " 'life',\n",
       " 'occupation',\n",
       " 'see',\n",
       " 'website',\n",
       " 'decide',\n",
       " 'men',\n",
       " 'women',\n",
       " 'all',\n",
       " 'criminal',\n",
       " 'minds',\n",
       " 'thoughts',\n",
       " 'destruction',\n",
       " 'mayhem',\n",
       " 'absolute',\n",
       " 'disservice',\n",
       " 'things',\n",
       " 'do',\n",
       " 'day',\n",
       " 'One',\n",
       " 'person',\n",
       " 'even',\n",
       " 'went',\n",
       " 'so',\n",
       " 'far',\n",
       " 'as',\n",
       " 'say',\n",
       " 'members',\n",
       " 'personal',\n",
       " 'gain',\n",
       " 'Wow',\n",
       " '!',\n",
       " 'Entry',\n",
       " 'level',\n",
       " 'personnel',\n",
       " 'just',\n",
       " 'under',\n",
       " '$',\n",
       " '8.00',\n",
       " 'assuming',\n",
       " '40',\n",
       " 'work',\n",
       " 'week',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'many',\n",
       " 'much',\n",
       " 'more',\n",
       " 'those',\n",
       " 'harm',\n",
       " 'way',\n",
       " 'typically',\n",
       " 'put',\n",
       " '16-18',\n",
       " 'end',\n",
       " 'pay',\n",
       " 'well',\n",
       " 'wage',\n",
       " 'So',\n",
       " 'beg',\n",
       " 'please',\n",
       " 'yourself',\n",
       " 'familiar',\n",
       " 'around',\n",
       " 'Go',\n",
       " 'nearby',\n",
       " 'visitor',\n",
       " 'pass',\n",
       " 'meet',\n",
       " 'some',\n",
       " 'quick',\n",
       " 'disparage',\n",
       " 'You',\n",
       " 'surprised',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'accepts',\n",
       " 'lieu',\n",
       " 'prison',\n",
       " 'They',\n",
       " 'require',\n",
       " 'GED',\n",
       " 'prefer',\n",
       " 'high',\n",
       " 'school',\n",
       " 'diploma',\n",
       " 'middle',\n",
       " 'ranks',\n",
       " 'expected',\n",
       " 'undergraduate',\n",
       " 'degrees',\n",
       " 'upper',\n",
       " 'encouraged',\n",
       " 'advanced']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수가 2 이하인 단어 리스트를 추출\n",
    "# items() 함수 활용 - 키와 값들의 쌍을 얻을 수 있게 해줌\n",
    "# (1)리스트 컴프리헨션\n",
    "low_freq_words = [key for key, value in vocab.items() if value <=2]\n",
    "low_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5b2fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['After',\n",
       " 'reading',\n",
       " 'comments',\n",
       " 'am',\n",
       " 'sure',\n",
       " 'whether',\n",
       " 'should',\n",
       " 'angry',\n",
       " 'sad',\n",
       " 'sickened',\n",
       " 'Seeing',\n",
       " 'typical',\n",
       " ')',\n",
       " 'know',\n",
       " 'absolutely',\n",
       " 'nothing',\n",
       " 'b',\n",
       " 'base',\n",
       " 'everything',\n",
       " 'think',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'CNN',\n",
       " 'reports',\n",
       " 'me',\n",
       " 'wonder',\n",
       " 'state',\n",
       " 'intellectual',\n",
       " 'stimulation',\n",
       " 'world',\n",
       " 'At',\n",
       " 'time',\n",
       " 'type',\n",
       " ':',\n",
       " '1.4',\n",
       " 'Active',\n",
       " 'Duty',\n",
       " 'another',\n",
       " 'almost',\n",
       " '900,000',\n",
       " 'Guard',\n",
       " 'Reserves',\n",
       " 'roughly',\n",
       " '2.3',\n",
       " 'indicted',\n",
       " 'abuses',\n",
       " 'Currently',\n",
       " 'less',\n",
       " 'than',\n",
       " '20',\n",
       " 'That',\n",
       " '.00083',\n",
       " '%',\n",
       " 'Even',\n",
       " 'if',\n",
       " 'indict',\n",
       " 'single',\n",
       " 'member',\n",
       " 'ever',\n",
       " 'stepped',\n",
       " 'come',\n",
       " 'close',\n",
       " 'making',\n",
       " 'whole',\n",
       " 'flaws',\n",
       " 'take',\n",
       " 'YEARS',\n",
       " 'cover',\n",
       " 'understand',\n",
       " \"'s\",\n",
       " 'supposed',\n",
       " 'sarcastic',\n",
       " 'but',\n",
       " 'reality',\n",
       " 'writer',\n",
       " 'director',\n",
       " 'trying',\n",
       " 'commentary',\n",
       " 'without',\n",
       " 'enemy',\n",
       " 'fight',\n",
       " 'In',\n",
       " 'has',\n",
       " 'been',\n",
       " 'its',\n",
       " 'busiest',\n",
       " 'when',\n",
       " 'conflicts',\n",
       " 'going',\n",
       " 'is',\n",
       " 'first',\n",
       " 'called',\n",
       " 'disaster',\n",
       " 'relief',\n",
       " 'humanitarian',\n",
       " 'aid',\n",
       " 'missions',\n",
       " 'When',\n",
       " 'tsunami',\n",
       " 'hit',\n",
       " 'Indonesia',\n",
       " 'devestating',\n",
       " 'region',\n",
       " 'scene',\n",
       " 'chaos',\n",
       " 'situation',\n",
       " 'overwhelmed',\n",
       " 'local',\n",
       " 'governments',\n",
       " 'leadership',\n",
       " 'looked',\n",
       " 'their',\n",
       " 'same',\n",
       " 'mocks',\n",
       " 'said',\n",
       " 'happen',\n",
       " 'Within',\n",
       " 'hours',\n",
       " 'food',\n",
       " 'reaching',\n",
       " 'isolated',\n",
       " 'villages',\n",
       " 'airfields',\n",
       " 'were',\n",
       " 'built',\n",
       " 'cargo',\n",
       " 'aircraft',\n",
       " 'started',\n",
       " 'landing',\n",
       " 'distribution',\n",
       " 'system',\n",
       " 'up',\n",
       " 'running',\n",
       " 'Hours',\n",
       " 'weeks',\n",
       " 'months',\n",
       " 'Yes',\n",
       " 'unscrupulous',\n",
       " 'But',\n",
       " 'then',\n",
       " 'walk',\n",
       " 'life',\n",
       " 'occupation',\n",
       " 'see',\n",
       " 'website',\n",
       " 'decide',\n",
       " 'men',\n",
       " 'women',\n",
       " 'all',\n",
       " 'criminal',\n",
       " 'minds',\n",
       " 'thoughts',\n",
       " 'destruction',\n",
       " 'mayhem',\n",
       " 'absolute',\n",
       " 'disservice',\n",
       " 'things',\n",
       " 'do',\n",
       " 'day',\n",
       " 'One',\n",
       " 'person',\n",
       " 'even',\n",
       " 'went',\n",
       " 'so',\n",
       " 'far',\n",
       " 'as',\n",
       " 'say',\n",
       " 'members',\n",
       " 'personal',\n",
       " 'gain',\n",
       " 'Wow',\n",
       " '!',\n",
       " 'Entry',\n",
       " 'level',\n",
       " 'personnel',\n",
       " 'just',\n",
       " 'under',\n",
       " '$',\n",
       " '8.00',\n",
       " 'assuming',\n",
       " '40',\n",
       " 'work',\n",
       " 'week',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'many',\n",
       " 'much',\n",
       " 'more',\n",
       " 'those',\n",
       " 'harm',\n",
       " 'way',\n",
       " 'typically',\n",
       " 'put',\n",
       " '16-18',\n",
       " 'end',\n",
       " 'pay',\n",
       " 'well',\n",
       " 'wage',\n",
       " 'So',\n",
       " 'beg',\n",
       " 'please',\n",
       " 'yourself',\n",
       " 'familiar',\n",
       " 'around',\n",
       " 'Go',\n",
       " 'nearby',\n",
       " 'visitor',\n",
       " 'pass',\n",
       " 'meet',\n",
       " 'some',\n",
       " 'quick',\n",
       " 'disparage',\n",
       " 'You',\n",
       " 'surprised',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'accepts',\n",
       " 'lieu',\n",
       " 'prison',\n",
       " 'They',\n",
       " 'require',\n",
       " 'GED',\n",
       " 'prefer',\n",
       " 'high',\n",
       " 'school',\n",
       " 'diploma',\n",
       " 'middle',\n",
       " 'ranks',\n",
       " 'expected',\n",
       " 'undergraduate',\n",
       " 'degrees',\n",
       " 'upper',\n",
       " 'encouraged',\n",
       " 'advanced']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수가 2 이하인 단어 리스트를 추출\n",
    "# (2)반복문을 활용해서 빈도수가 2 이하인 단어 리스트를 추출\n",
    "low_freq_words= []\n",
    "for key, value in vocab.items():\n",
    "    if value < 3:\n",
    "        low_freq_words.append(key)\n",
    "        \n",
    "low_freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ccdcd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수가 2 이하인 단어수 :  234\n"
     ]
    }
   ],
   "source": [
    "# 등장반도가 2 이하인 단어 리스트 개수 확인\n",
    "print(\"빈도수가 2 이하인 단어수 : \",len(low_freq_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4200b652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'for',\n",
       " 'this',\n",
       " 'movie',\n",
       " ',',\n",
       " 'I',\n",
       " 'not',\n",
       " 'I',\n",
       " 'be',\n",
       " ',',\n",
       " 'or',\n",
       " '.',\n",
       " 'of',\n",
       " 'people',\n",
       " 'who',\n",
       " 'a',\n",
       " 'about',\n",
       " 'the',\n",
       " 'military',\n",
       " 'or',\n",
       " 'who',\n",
       " 'they',\n",
       " 'they',\n",
       " 'on',\n",
       " 'this',\n",
       " 'or',\n",
       " 'on',\n",
       " 'about',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'about',\n",
       " 'the',\n",
       " 'of',\n",
       " 'in',\n",
       " 'the',\n",
       " '.',\n",
       " 'the',\n",
       " 'I',\n",
       " 'this',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'people',\n",
       " 'in',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " 'million',\n",
       " 'on',\n",
       " 'with',\n",
       " 'in',\n",
       " 'the',\n",
       " 'and',\n",
       " 'for',\n",
       " 'a',\n",
       " 'total',\n",
       " 'of',\n",
       " 'million',\n",
       " '.',\n",
       " 'The',\n",
       " 'number',\n",
       " 'of',\n",
       " 'people',\n",
       " 'for',\n",
       " 'at',\n",
       " 'at',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'total',\n",
       " 'of',\n",
       " 'people',\n",
       " 'of',\n",
       " 'the',\n",
       " 'total',\n",
       " 'military',\n",
       " '.',\n",
       " 'you',\n",
       " 'every',\n",
       " 'military',\n",
       " 'that',\n",
       " 'in',\n",
       " 'to',\n",
       " 'Abu-Gharib',\n",
       " ',',\n",
       " 'you',\n",
       " 'would',\n",
       " 'not',\n",
       " 'to',\n",
       " 'that',\n",
       " 'a',\n",
       " 'number',\n",
       " '.',\n",
       " 'The',\n",
       " 'in',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'would',\n",
       " 'to',\n",
       " '.',\n",
       " 'I',\n",
       " 'that',\n",
       " 'it',\n",
       " 'to',\n",
       " 'be',\n",
       " ',',\n",
       " 'in',\n",
       " ',',\n",
       " 'the',\n",
       " 'and',\n",
       " 'are',\n",
       " 'to',\n",
       " 'make',\n",
       " 'about',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'military',\n",
       " 'an',\n",
       " 'to',\n",
       " '.',\n",
       " ',',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " 'at',\n",
       " 'there',\n",
       " 'are',\n",
       " 'not',\n",
       " 'on',\n",
       " '.',\n",
       " 'The',\n",
       " 'military',\n",
       " 'the',\n",
       " 'for',\n",
       " 'and',\n",
       " '.',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " 'was',\n",
       " 'the',\n",
       " 'on',\n",
       " 'the',\n",
       " '.',\n",
       " 'the',\n",
       " 'of',\n",
       " 'the',\n",
       " 'the',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'military',\n",
       " 'who',\n",
       " 'at',\n",
       " 'people',\n",
       " ',',\n",
       " 'the',\n",
       " 'people',\n",
       " 'this',\n",
       " 'movie',\n",
       " ',',\n",
       " 'and',\n",
       " 'make',\n",
       " 'it',\n",
       " '.',\n",
       " ',',\n",
       " 'was',\n",
       " '.',\n",
       " 'days',\n",
       " ',',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'was',\n",
       " 'and',\n",
       " '.',\n",
       " 'and',\n",
       " 'days',\n",
       " ',',\n",
       " 'not',\n",
       " 'and',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'in',\n",
       " 'the',\n",
       " 'US',\n",
       " 'military',\n",
       " '.',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'in',\n",
       " 'every',\n",
       " 'of',\n",
       " ',',\n",
       " 'every',\n",
       " '.',\n",
       " 'to',\n",
       " 'people',\n",
       " 'on',\n",
       " 'this',\n",
       " 'that',\n",
       " 'million',\n",
       " 'and',\n",
       " 'are',\n",
       " ',',\n",
       " 'with',\n",
       " 'on',\n",
       " 'of',\n",
       " 'or',\n",
       " 'an',\n",
       " 'to',\n",
       " 'the',\n",
       " 'that',\n",
       " 'they',\n",
       " 'every',\n",
       " '.',\n",
       " 'on',\n",
       " 'this',\n",
       " 'to',\n",
       " 'that',\n",
       " 'military',\n",
       " 'are',\n",
       " 'in',\n",
       " 'it',\n",
       " 'for',\n",
       " '.',\n",
       " 'make',\n",
       " 'an',\n",
       " 'hour',\n",
       " 'a',\n",
       " 'hour',\n",
       " '.',\n",
       " ',',\n",
       " 'a',\n",
       " 'and',\n",
       " 'in',\n",
       " 'in',\n",
       " 'hour',\n",
       " 'days',\n",
       " 'for',\n",
       " 'on',\n",
       " '.',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'minimum',\n",
       " '.',\n",
       " 'for',\n",
       " '.',\n",
       " 'I',\n",
       " 'you',\n",
       " ',',\n",
       " 'make',\n",
       " 'with',\n",
       " 'the',\n",
       " 'you',\n",
       " '.',\n",
       " 'to',\n",
       " 'a',\n",
       " ',',\n",
       " 'get',\n",
       " 'a',\n",
       " 'and',\n",
       " 'of',\n",
       " 'the',\n",
       " 'and',\n",
       " 'you',\n",
       " 'are',\n",
       " 'to',\n",
       " '.',\n",
       " 'would',\n",
       " 'be',\n",
       " '.',\n",
       " 'The',\n",
       " 'military',\n",
       " 'people',\n",
       " 'in',\n",
       " 'of',\n",
       " '.',\n",
       " 'a',\n",
       " 'minimum',\n",
       " 'of',\n",
       " 'a',\n",
       " 'and',\n",
       " 'a',\n",
       " '.',\n",
       " 'The',\n",
       " 'are',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'minimum',\n",
       " 'of',\n",
       " 'and',\n",
       " 'the',\n",
       " 'are',\n",
       " 'to',\n",
       " 'get',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수가 2 이하인 단어들을 제거한 결과를 따로 저장(cleaned_by_freq 변수에 저장)\n",
    "# in, not in, 반복문(for), 조건문(if),append\n",
    "cleaned_by_freq= []\n",
    "for word in tokenized_words:\n",
    "    if word not in low_freq_words:\n",
    "        cleaned_by_freq.append(word)\n",
    "cleaned_by_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d507d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수가 3 이상인 단어수 :  306\n"
     ]
    }
   ],
   "source": [
    "print(\"빈도수가 3 이상인 단어수 : \",len(cleaned_by_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eb88017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'for',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'not',\n",
       " 'people',\n",
       " 'who',\n",
       " 'about',\n",
       " 'the',\n",
       " 'military',\n",
       " 'who',\n",
       " 'they',\n",
       " 'they',\n",
       " 'this',\n",
       " 'about',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'about',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'this',\n",
       " 'the',\n",
       " 'number',\n",
       " 'people',\n",
       " 'the',\n",
       " 'military',\n",
       " 'million',\n",
       " 'with',\n",
       " 'the',\n",
       " 'and',\n",
       " 'for',\n",
       " 'total',\n",
       " 'million',\n",
       " 'The',\n",
       " 'number',\n",
       " 'people',\n",
       " 'for',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'total',\n",
       " 'people',\n",
       " 'the',\n",
       " 'total',\n",
       " 'military',\n",
       " 'you',\n",
       " 'every',\n",
       " 'military',\n",
       " 'that',\n",
       " 'Abu-Gharib',\n",
       " 'you',\n",
       " 'would',\n",
       " 'not',\n",
       " 'that',\n",
       " 'number',\n",
       " 'The',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'would',\n",
       " 'that',\n",
       " 'the',\n",
       " 'and',\n",
       " 'are',\n",
       " 'make',\n",
       " 'about',\n",
       " 'the',\n",
       " 'the',\n",
       " 'military',\n",
       " 'the',\n",
       " 'military',\n",
       " 'there',\n",
       " 'are',\n",
       " 'not',\n",
       " 'The',\n",
       " 'military',\n",
       " 'the',\n",
       " 'for',\n",
       " 'and',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'military',\n",
       " 'was',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'was',\n",
       " 'military',\n",
       " 'who',\n",
       " 'people',\n",
       " 'the',\n",
       " 'people',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'make',\n",
       " 'was',\n",
       " 'days',\n",
       " 'and',\n",
       " 'was',\n",
       " 'and',\n",
       " 'and',\n",
       " 'days',\n",
       " 'not',\n",
       " 'and',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'the',\n",
       " 'military',\n",
       " 'there',\n",
       " 'are',\n",
       " 'every',\n",
       " 'every',\n",
       " 'people',\n",
       " 'this',\n",
       " 'that',\n",
       " 'million',\n",
       " 'and',\n",
       " 'are',\n",
       " 'with',\n",
       " 'the',\n",
       " 'that',\n",
       " 'they',\n",
       " 'every',\n",
       " 'this',\n",
       " 'that',\n",
       " 'military',\n",
       " 'are',\n",
       " 'for',\n",
       " 'make',\n",
       " 'hour',\n",
       " 'hour',\n",
       " 'and',\n",
       " 'hour',\n",
       " 'days',\n",
       " 'for',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'minimum',\n",
       " 'for',\n",
       " 'you',\n",
       " 'make',\n",
       " 'with',\n",
       " 'the',\n",
       " 'you',\n",
       " 'get',\n",
       " 'and',\n",
       " 'the',\n",
       " 'and',\n",
       " 'you',\n",
       " 'are',\n",
       " 'would',\n",
       " 'The',\n",
       " 'military',\n",
       " 'people',\n",
       " 'minimum',\n",
       " 'and',\n",
       " 'The',\n",
       " 'are',\n",
       " 'get',\n",
       " 'minimum',\n",
       " 'and',\n",
       " 'the',\n",
       " 'are',\n",
       " 'get']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이가 2이하인 단어 제거\n",
    "# 콤마, I, be, . 등 의미를 갖지 않는 단어 제거\n",
    "cleaned_by_freq_len = []\n",
    "# cleaned_by_freq에서 길이가 2이하인 단어 제거\n",
    "# 제거된 값은 cleaned_by_freq_len 이라는 변수에 담기\n",
    "for word in cleaned_by_freq:\n",
    "    if len(word) >= 3:\n",
    "        cleaned_by_freq_len.append(word)\n",
    "cleaned_by_freq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e21513a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정제 전 ['the', 'for', 'this', 'movie', ',', 'I', 'not', 'I', 'be', ',']\n",
      "정제 후 ['the', 'for', 'this', 'movie', 'not', 'people', 'who', 'about', 'the', 'military']\n"
     ]
    }
   ],
   "source": [
    "# 정제 결과 확인\n",
    "print('정제 전',cleaned_by_freq[:10])\n",
    "print('정제 후',cleaned_by_freq_len[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30622fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b273f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3316cdc3",
   "metadata": {},
   "source": [
    "### 정제함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c232c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 등장 빈도 기준 정제 함수 정의\n",
    "\n",
    "def clean_by_freq(tokenized_words, freq):\n",
    "    # 1. Counter 함수를 통해 단어의 빈도수를 카운트하여 단어 집합 생성\n",
    "    # Vocab 이라는 변수에 담기\n",
    "    vocab = Counter(tokenized_words)\n",
    "    # 2. 빈도수가 freq 이하인 단어 추출\n",
    "    # low_freq_words 라는 변수에 담기\n",
    "    low_freq_words= []\n",
    "    for key, value in vocab.items():\n",
    "        if value < freq:\n",
    "            low_freq_words.append(key)\n",
    "    # 3. low_freq_words에 포함되지 않는 단어 리스트 생성\n",
    "    # cleaned_words 라는 변수에 담기\n",
    "    cleaned_words= []\n",
    "    for word in tokenized_words:\n",
    "        if word not in low_freq_words:\n",
    "            cleaned_words.append(word)\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2676d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 길이 기준 정제함수\n",
    "def clean_by_len(tokenized_words, length):\n",
    "    cleaned_by_freq_len = []\n",
    "    # 단어 길이가 length 이상인 단어들을 cleaned_by_freq_len 이라는 변수에 담기\n",
    "    for word in tokenized_words:\n",
    "        if len(word) >= length:\n",
    "            cleaned_by_freq_len.append(word)\n",
    "    return cleaned_by_freq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5662907",
   "metadata": {},
   "source": [
    "### 불용어 ( Stopwords)\n",
    "- 코퍼스에서 큰 의미가 없거나, 분석 목적에서 벗어나는 단어들\n",
    "\n",
    "1. 불용어를 모아 놓은 불용어 세트 준비\n",
    "2. 코퍼스의 각 단어들이 불용어 세트에 포함이 되는지 확인\n",
    "3. 불용어 세트에 있는 단어 토큰은 분석에서 제외(삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1ca6f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smhrd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 코퍼스의 종류에 상관없이 많이 사용되는 기본 불용어 목록\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a2da1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기본 불용어 개수 :  179\n",
      "{'himself', 'mightn', 'then', 'we', 'o', 'she', 'nor', 'her', 'itself', 'has', 'about', 'for', 'd', 'not', 'mustn', \"needn't\", 'against', 'your', 'these', \"shan't\", 'he', 'no', 'out', \"shouldn't\", 'yours', 'am', 'between', \"doesn't\", 'wouldn', 'those', 'again', 'there', 'but', 'my', 'it', 'below', 'any', 'haven', 'above', 'up', 'under', 'this', 'after', 'ain', 'have', 'wasn', 'doesn', 'is', 'than', 'will', 'all', 'be', 'what', 'because', 'by', 'their', 'both', \"that'll\", 'didn', 'ma', 'had', 'some', \"didn't\", 'so', 'or', 'were', 'at', 'too', 'now', 'just', \"weren't\", 'into', 'same', 'as', 'only', 'other', 'me', \"isn't\", 'theirs', 'hadn', 'until', 'off', 'from', 'his', 'yourself', 'our', 'down', 'you', 'the', 'when', 'where', \"hadn't\", \"won't\", \"wouldn't\", \"it's\", 're', 'ours', 'how', \"wasn't\", 'each', 'whom', 'its', 'which', 'do', 'with', 'can', 'don', 'on', 'y', 'weren', 'doing', 'who', 'was', 'further', \"aren't\", 't', 'needn', 'shouldn', 'over', 'him', 'most', 'here', 'shan', 'in', 'i', 'being', 'themselves', \"couldn't\", 'll', 'an', 'once', 'own', 'couldn', 'such', 've', 'does', 'they', \"don't\", 'through', \"should've\", 'myself', 'if', 's', 'few', 'isn', 'ourselves', 'yourselves', 'are', 'and', 'won', \"you'd\", \"you'll\", 'having', \"haven't\", 'aren', \"mustn't\", 'to', 'been', 'did', \"you're\", 'before', 'hasn', 'during', 'them', 'that', 'why', \"she's\", \"mightn't\", 'very', 'while', \"hasn't\", \"you've\", 'of', 'more', 'hers', 'm', 'a', 'herself', 'should'}\n"
     ]
    }
   ],
   "source": [
    "# 불용어 세트 확인\n",
    "# 분석에 크게 활용되지 않는 불용어 세트\n",
    "# 집합 자료형(set) : 중복을 허용하지 않는 자료형 / 순서가 존재 X\n",
    "stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "# 불용어 개수\n",
    "# 179개의 기본 불용어 목록 제공\n",
    "print('기본 불용어 개수 : ',len(stopwords_set))\n",
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0dd3372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 :  178\n",
      "{'himself', 'mightn', 'then', 'we', 'o', 'she', 'nor', 'hello', 'her', 'itself', 'has', 'about', 'for', 'd', 'not', 'mustn', \"needn't\", 'against', 'your', 'these', \"shan't\", 'he', 'no', 'out', \"shouldn't\", 'yours', 'am', 'between', \"doesn't\", 'wouldn', 'those', 'again', 'there', 'but', 'it', 'below', 'any', 'haven', 'above', 'up', 'under', 'this', 'ain', 'have', 'wasn', 'doesn', 'is', 'than', 'will', 'all', 'be', 'what', 'because', 'by', 'their', 'both', \"that'll\", 'didn', 'ma', 'had', 'some', \"didn't\", 'so', 'or', 'were', 'at', 'too', 'now', 'just', \"weren't\", 'into', 'same', 'as', 'only', 'other', 'me', \"isn't\", 'theirs', 'hadn', 'until', 'off', 'from', 'his', 'yourself', 'our', 'down', 'you', 'the', 'when', 'where', \"hadn't\", \"won't\", \"wouldn't\", \"it's\", 're', 'ours', 'how', \"wasn't\", 'each', 'whom', 'its', 'which', 'do', 'with', 'can', 'don', 'on', 'y', 'weren', 'doing', 'who', 'was', 'further', \"aren't\", 't', 'needn', 'shouldn', 'over', 'him', 'most', 'here', 'shan', 'in', 'i', 'being', 'themselves', \"couldn't\", 'll', 'an', 'once', 'own', 'couldn', 'such', 've', 'does', 'they', \"don't\", 'through', \"should've\", 'myself', 'if', 's', 'few', 'isn', 'ourselves', 'yourselves', 'are', 'and', 'won', \"you'd\", \"you'll\", 'having', \"haven't\", 'aren', \"mustn't\", 'to', 'been', 'did', \"you're\", 'before', 'hasn', 'during', 'them', 'that', 'why', \"she's\", \"mightn't\", 'very', 'while', \"hasn't\", \"you've\", 'of', 'more', 'hers', 'm', 'a', 'herself', 'should'}\n"
     ]
    }
   ],
   "source": [
    "# 불용어 목록에 새로운 단어 추가, 제거\n",
    "# add(), remove()\n",
    "\n",
    "stopwords_set.add('hello')\n",
    "stopwords_set.remove('after')\n",
    "stopwords_set.remove('my')\n",
    "print('불용어 개수 : ',len(stopwords_set))\n",
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bae0ef66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'each', 'whom', 'its', 'mightn', 'then', 'o', 'nor', 'can', 'on', 'y', 'weren', 'has', 'doing', \"aren't\", 't', 'about', 'd', 'shouldn', 'mustn', 'over', 'him', 'against', 'here', 'he', 'i', 'themselves', 'out', \"shouldn't\", 'll', 'yours', 'those', 'again', 'but', 'below', 'above', 'few', 'after', 'ain', 'have', 'isn', 'is', 'yourselves', 'are', 'and', 'had', 'so', \"you'll\", 'having', 'at', \"haven't\", 'now', 'just', \"mustn't\", 'into', 'to', 'same', 'been', 'before', 'other', 'me', \"isn't\", 'off', 'that', \"mightn't\", 'yourself', 'down', 'a', 'the', 'more', \"hadn't\", 'how', 'herself'}\n"
     ]
    }
   ],
   "source": [
    "# 사용자가 직접 불용어 세트를 정의\n",
    "\n",
    "my_stopwords_set = {'the', \"isn't\", 'him', 'weren', 'so', 'has', 'again', 'have', 'out', 'mustn', 'to', \"shouldn't\", 'against', 'after', 'off', 'can', 'about', 'whom', 'above', \"mightn't\", 'mightn', 'each', 'been', 'yourself', 'themselves', 'had', 'is', \"hadn't\", 'those', 'a', \"you'll\", 'over', 'that', 'how', \"aren't\", 'doing', 'y', 'herself', 'now', 'd', 'other', 'are', 'ain', 'having', 'its', \"mustn't\", 'he', 'then', 'yours', 'shouldn', 'but', 'on', 'more', 'i', 'yourselves', 'same', 'at', 'nor', 'isn', \"haven't\", 't', 'below', 'll', 'down', 'few', 'just', 'before', 'into', 'and', 'o', 'me', 'here'}\n",
    "print(my_stopwords_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ce831",
   "metadata": {},
   "source": [
    "# 불용어 (Stopwords) 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ad6c024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movie',\n",
       " 'people',\n",
       " 'military',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'number',\n",
       " 'people',\n",
       " 'military',\n",
       " 'million',\n",
       " 'total',\n",
       " 'million',\n",
       " 'The',\n",
       " 'number',\n",
       " 'people',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'total',\n",
       " 'people',\n",
       " 'total',\n",
       " 'military',\n",
       " 'every',\n",
       " 'military',\n",
       " 'Abu-Gharib',\n",
       " 'would',\n",
       " 'number',\n",
       " 'The',\n",
       " 'movie',\n",
       " 'would',\n",
       " 'make',\n",
       " 'military',\n",
       " 'military',\n",
       " 'The',\n",
       " 'military',\n",
       " 'military',\n",
       " 'military',\n",
       " 'people',\n",
       " 'people',\n",
       " 'movie',\n",
       " 'make',\n",
       " 'days',\n",
       " 'days',\n",
       " 'people',\n",
       " 'military',\n",
       " 'every',\n",
       " 'every',\n",
       " 'people',\n",
       " 'million',\n",
       " 'every',\n",
       " 'military',\n",
       " 'make',\n",
       " 'hour',\n",
       " 'hour',\n",
       " 'hour',\n",
       " 'days',\n",
       " 'makes',\n",
       " 'minimum',\n",
       " 'make',\n",
       " 'get',\n",
       " 'would',\n",
       " 'The',\n",
       " 'military',\n",
       " 'people',\n",
       " 'minimum',\n",
       " 'The',\n",
       " 'get',\n",
       " 'minimum',\n",
       " 'get']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 등장빈도와 단어 길이를 기준으로 정제했던 변수 :cleaned_by_freq_len\n",
    "cleaned_by_freq_len\n",
    "\n",
    "# cleaned_by_freq_len 토큰을 하나씩 확인하면서 단어가 불용어 세트에\n",
    "# 존재하지 않을 때에만 cleaned_words 변수에 값 담아주기\n",
    "cleaned_words =[]\n",
    "for word in cleaned_by_freq_len:\n",
    "     if word not in stopwords_set:\n",
    "        cleaned_words.append(word)\n",
    "cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f3093",
   "metadata": {},
   "source": [
    "### 불용어 처리 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fca1119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 함수 만들기\n",
    "def clean_by_stopwords(tokenized_words,stopwords_set):\n",
    "    cleaned_words = []\n",
    "    \n",
    "    # 불용어 제거하는 코드 작성\n",
    "    for word in tokenized_words:\n",
    "        if word not in stopwords_set:\n",
    "             cleaned_words.append(word)\n",
    "                \n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46caf4d",
   "metadata": {},
   "source": [
    "### 정규화(Normalizaton)\n",
    "- The == the ->통합\n",
    "- USA, America, United State America, U.S.A ...\n",
    "- 형태가 다르지만 같은 의미를 나타내는 단어들이 많을 수록 분석이 어려워짐\n",
    "- 의미가 같은 단어라면 형태를 하나로 통일 -> 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929ae10",
   "metadata": {},
   "source": [
    "#### 정규화방법1 :대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dbadae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what can i do for you? do your homework now.\n"
     ]
    }
   ],
   "source": [
    "text = \"What can I do for you? Do your homework now.\"\n",
    "\n",
    "# 소문자로 변환() - lower() cf)대문자 upper()\n",
    "print(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c433d6e",
   "metadata": {},
   "source": [
    "#### 정규화방법2: 규칙 기반 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fcf4293",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"She became a US citizen. Ummmm, I think, maybe and or.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8f612cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'became', 'a', 'USA', 'citizen', '.', 'Umm', ',', 'I', 'think', ',', 'maybe', 'and', 'or', '.']\n"
     ]
    }
   ],
   "source": [
    "# 동의어 사전 만들어서 정규화 진행\n",
    "# US를 USA, Ummmm을 Umm으로 통합\n",
    "synonym_dict = {'US' : 'USA', 'U.S' : 'USA', 'Ummmm' : 'Umm', 'Ummm' : 'Umm'}\n",
    "\n",
    "#단어 토큰화\n",
    "tokenized_words =  word_tokenize(text)\n",
    "\n",
    "normalized_words = []\n",
    "\n",
    "# 동의어 사전에 있는 단어라면, value에 해당하는 값으로 변환\n",
    "\n",
    "for word in tokenized_words : \n",
    "    if word in synonym_dict.keys() : \n",
    "        word = synonym_dict[word]\n",
    "    normalized_words.append(word)\n",
    "print(normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982faae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece461f1",
   "metadata": {},
   "source": [
    "### 어간 추출\n",
    "- 어간(Stem) : 특정한 단어의 핵심이 되는 부분\n",
    "- 어간추출(Stemming) : 단어에서 어간을 찾아내는 것\n",
    "- 어간추출을 하면 같은 단어로 통합되기 떄문에 정규화 방법 중 하나임  \n",
    "- Formalize -> Formal(alize -> al)\n",
    "- Relational -> Relate(ational->ate)\n",
    "- Activate -> Activ(ate -> 제거)  \n",
    "\n",
    "    - 코퍼스의 특성이나 상황에 따라 어간 추출을 하는게 적합할지를 판단\n",
    "    - 분석에 활용해야하는 중요한 단어가 손실될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ee6fd",
   "metadata": {},
   "source": [
    "#### NLTK로 어간 추출하기\n",
    "- 어간 추출을 위한 알고리즘\n",
    "    - 포터 스테머(Poter Stemmer)\n",
    "    - 랭커스터 스테머(Lacaster Stemmer)\n",
    "    - 어간을 추출하는 기준이 미세하게 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a136c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c6022c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48212a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You are so lovely. I am loving you now.\"\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words =  word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c9fa93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lovely'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_words[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4342f8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 포터스테머 활용\n",
    "porter_stemmer.stem(tokenized_words[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e4a9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer_words=[]\n",
    "\n",
    "# 포터스테머 어간 추출\n",
    "# 토큰화된 단어를 반복하면서 어간을 추출한다\n",
    "# 값을 porter_stemmer_words 에 담아주기\n",
    "for word in tokenized_words :\n",
    "    stem = porter_stemmer.stem(word)\n",
    "    porter_stemmer_words.append(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "198f0da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "print(porter_stemmer_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7f3910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간추출 전 ['You', 'are', 'so', 'lovely', '.', 'I', 'am', 'loving', 'you', 'now', '.']\n",
      "어간추출 후 ['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "# 어간추출 전/후 비교\n",
    "print('어간추출 전',tokenized_words)\n",
    "print('어간추출 후',porter_stemmer_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b901325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afb54a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랭커스터 스테머 객체 생성\n",
    "lancaster_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e52fa359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 토큰화\n",
    "text = \"You are so lovely. I am loving you now.\"\n",
    "tokenized_words =  word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c46fa148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 돌아가면서 어간추출 진행\n",
    "lancaster_stemmed_words = []\n",
    "\n",
    "for word in tokenized_words :\n",
    "    stem = lancaster_stemmer.stem(word)\n",
    "    lancaster_stemmed_words.append(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43328779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'ar', 'so', 'lov', '.', 'i', 'am', 'lov', 'you', 'now', '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster_stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ed95c5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간추출 전 ['You', 'are', 'so', 'lovely', '.', 'I', 'am', 'loving', 'you', 'now', '.']\n",
      "포터스테머 어간추출 후 ['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n",
      "랭커스터스테머 어간추출 후 ['you', 'ar', 'so', 'lov', '.', 'i', 'am', 'lov', 'you', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "# 어간추출 전/후 비교\n",
    "print('어간추출 전',tokenized_words)\n",
    "print('포터스테머 어간추출 후',porter_stemmer_words)\n",
    "print('랭커스터스테머 어간추출 후',lancaster_stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4296a70",
   "metadata": {},
   "source": [
    "### 어간 추출 함수 코드화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "462feac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 포터스테머 어간 추출 함수\n",
    "def stemming_by_porter(tokenized_words):\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    porter_stemmed_words =[]\n",
    "    for word in tokenized_words :\n",
    "        stem = porter_stemmer.stem(word)\n",
    "        porter_stemmed_words.append(stem)\n",
    "    return porter_stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134e998",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "- 단어의 품사는 문장 안에서 사용되는 위치에 따라 달라질 수 있기 떄문에 문장 토큰화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03a3fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수와 모듈 불러오기\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2a9803e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My email address is 'abcde@smhrd.com'.\", 'Send it to Mr.Kim.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My email address is 'abcde@smhrd.com'. Send it to Mr.Kim.\"\n",
    "\n",
    "# 문장 토큰화\n",
    "# 약어와 같은 (Mr.,Dr.) 언어적인 특성을 고려해서 토큰화 \n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5862508e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can you forward my email to Mr.Kim?', 'Thank you!']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Can you forward my email to Mr.Kim? Thank you!\"\n",
    "\n",
    "# 느낌표, 물음표 등도 문장을 나누는 기준으로 적용\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142b198",
   "metadata": {},
   "source": [
    "### 품사 태깅(POS ; Part of Speech Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929867c6",
   "metadata": {},
   "source": [
    "- 각 단어가 어떤 품사로 쓰였는지 표시하는 작업 = 품사태깅\n",
    "- 여러문장코퍼스 -> 문장 토큰화 -> 단어 토큰화 -> 품사태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fc5f6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\smhrd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "787e2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사태깅에 필요한 도구 불러오기\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "846f330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅을 할 코퍼스를 문장 기준으로 토큰화\n",
    "text = \"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\"Hey, let\\'s pool our money together and make a really bad movie!\\\" Or something like that.\"\n",
    "\n",
    "tokenized_sents= sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05b066aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 문장들을 반복하면서 단어 토큰화와 품사 태깅 작업진행\n",
    "pos_tagged_list = []\n",
    "\n",
    "\n",
    "for sentence in tokenized_sents:\n",
    "    # 단어 토큰화\n",
    "    tokenized_words = word_tokenize(sentence)\n",
    "    # 품사 태깅\n",
    "    pos_tagged = pos_tag(tokenized_words)\n",
    "    # pos_tagged_list에 값 담아주기\n",
    "    # 리스트로 끊어진 문장의 구분 없애기 -extend 활용\n",
    "    pos_tagged_list.extend(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "438b3c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Watching', 'VBG'),\n",
       " ('Time', 'NNP'),\n",
       " ('Chasers', 'NNPS'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('obvious', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('made', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('bunch', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('friends', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Maybe', 'RB'),\n",
       " ('they', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('sitting', 'VBG'),\n",
       " ('around', 'IN'),\n",
       " ('one', 'CD'),\n",
       " ('day', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('film', 'NN'),\n",
       " ('school', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('said', 'VBD'),\n",
       " (',', ','),\n",
       " ('``', '``'),\n",
       " ('Hey', 'NNP'),\n",
       " (',', ','),\n",
       " ('let', 'VB'),\n",
       " (\"'s\", 'POS'),\n",
       " ('pool', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('money', 'NN'),\n",
       " ('together', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('make', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('really', 'RB'),\n",
       " ('bad', 'JJ'),\n",
       " ('movie', 'NN'),\n",
       " ('!', '.'),\n",
       " (\"''\", \"''\"),\n",
       " ('Or', 'CC'),\n",
       " ('something', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [(단어, 품사태그),(단어, 품사태그)] 형태로 품사 태깅된 결과 반환\n",
    "# Penn Treebank POS Tags\n",
    "pos_tagged_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c98337",
   "metadata": {},
   "source": [
    "- 품사태그 내용 정리\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeccf99",
   "metadata": {},
   "source": [
    "### 품사 태깅 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13f47294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# 품사 태깅 함수 정의\n",
    "def pos_tagger(tokenized_sents):\n",
    "    pos_tagged_words = []\n",
    "    for sentence in tokenized_sents:\n",
    "        # 단어 토큰화\n",
    "        tokenized_words = word_tokenize(sentence)\n",
    "        \n",
    "        # 품사 태깅\n",
    "        pos_tagged = pos_tag(tokenized_words)\n",
    "        \n",
    "        # 품사태깅한 데이터 담아주기 -extend 활용\n",
    "        pos_tagged_words.extend(pos_tagged)\n",
    "    return pos_tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a26b71",
   "metadata": {},
   "source": [
    "### 표제어 추출\n",
    "- 표제어를 기준으로 통합하면 단어 정규화가 이루어짐\n",
    "- 단어의 근본적인 의미로 통합해주기\n",
    "- am, are, is -> be\n",
    "- happiest,happier -> happy\n",
    "- 표제어 추출에는 WordNet POS Tag를 사용하여 품사 태깅 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "797be9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('You', 'PRP'), ('are', 'VBP'), ('the', 'DT'), ('happiest', 'JJS'), ('person', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 표제어 추출을 하기 위한 품사 태깅 진행\n",
    "text = 'You are the happiest person.'\n",
    "\n",
    "# 단어 토큰화\n",
    "tokenized_words = word_tokenize(text)\n",
    "# 품사태그\n",
    "tagged_words = pos_tag(tokenized_words)\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6488d",
   "metadata": {},
   "source": [
    "### WordNet Pos Tag : 영어 어휘 데이터베이스에 적용되어 있는 품사 태그\n",
    "- N으로 시작하면 wn.NOUN\n",
    "- J로 시작하면 wn.ADJ\n",
    "- R로 시작하면 wn.ADV\n",
    "- V로 시작하면 wn.VERB 로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "557ead2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d3b0c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pennTreePosTag를 WordNetPosTag로 변환\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f8e4e48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\smhrd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\smhrd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표제어 추출을 위한 함수 호출\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d53ea9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어 추출하기 위한 WordNetLemmatizer 클래스에 있는\n",
    "# lemmatizer() 함수 호출\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [] # 표제어가 추출된 단어를 담을 리스트 생성\n",
    "\n",
    "for word, tag in tagged_words:\n",
    "    # WordNet Pos Tag로 변환하는 함수 호출\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    \n",
    "    # 품사를 기준으로 표제어 추출\n",
    "    if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag)) # 표제어 추출 함수\n",
    "    else :\n",
    "        lemmatized_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b8dae22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "표제어 추출 전 ['You', 'are', 'the', 'happiest', 'person', '.']\n",
      "표제어 추출 후 ['You', 'be', 'the', 'happy', 'person', '.']\n"
     ]
    }
   ],
   "source": [
    "print('표제어 추출 전',tokenized_words)\n",
    "print('표제어 추출 후',lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56731b5d",
   "metadata": {},
   "source": [
    "### 표제어 추출함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d544ceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\smhrd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\smhrd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# pennTree -> WordNet으로 변환\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "# 표제어 추출해주는 함수 정의\n",
    "def word_lemmatizer(pos_tagged_words):\n",
    "    lemmatizer = WordNetLemmatizer() # 객체 생성\n",
    "    lemmatized_words = [] # 표제어 추출된 단어를 담는 리스트\n",
    "    \n",
    "    for word, tag in pos_tagged_words:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        \n",
    "        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, wn_tag)) # 표제어 추출 함수\n",
    "        else :\n",
    "            lemmatized_words.append(word)           \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc19407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1900915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaada27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa844d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64b0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
