{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af92f765",
   "metadata": {},
   "source": [
    "## 목표\n",
    "- IMDb 영화 리뷰 데이터 The Internet Movie DataBase의 약자\n",
    "- 자연어 전처리 과정을 실습\n",
    "- 영화리뷰데이터를 감성분석 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b4482d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "33e88f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 토큰화 필요한 함수 호출\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 단어 빈도 계산에 사용할 Counter()함수 호출\n",
    "from collections import Counter # 단어의 빈도수를 세어주는 함수\n",
    "from nltk.corpus import stopwords # 불용어 관련 함수\n",
    "from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dfd2973c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Minor Spoilers In New York, Joan Barnard (Elvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yes, I agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Jennifer Ehle was sparkling in \\\"Pride and Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Amy Poehler is a terrific comedian on Saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>A plane carrying employees of a large biotech ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>A well made, gritty science fiction movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Incredibly dumb and utterly predictable story ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review\n",
       "0           0  Watching Time Chasers, it obvious that it was ...\n",
       "1           1  I saw this film about 20 years ago and remembe...\n",
       "2           2  Minor Spoilers In New York, Joan Barnard (Elvi...\n",
       "3           3  I went to see this film with a great deal of e...\n",
       "4           4  Yes, I agree with everyone on this site this m...\n",
       "5           5  Jennifer Ehle was sparkling in \\\"Pride and Pre...\n",
       "6           6  Amy Poehler is a terrific comedian on Saturday...\n",
       "7           7  A plane carrying employees of a large biotech ...\n",
       "8           8  A well made, gritty science fiction movie, it ...\n",
       "9           9  Incredibly dumb and utterly predictable story ..."
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "# delimiter(구분자) 속성을 '\\t'로 지정\n",
    "df = pd.read_csv('imdb.tsv', delimiter=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d07dc",
   "metadata": {},
   "source": [
    "###  대소문자 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "272ab724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소문자로 변환 / lower() \n",
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c0a7c071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review\n",
       "0           0  watching time chasers, it obvious that it was ...\n",
       "1           1  i saw this film about 20 years ago and remembe...\n",
       "2           2  minor spoilers in new york, joan barnard (elvi...\n",
       "3           3  i went to see this film with a great deal of e...\n",
       "4           4  yes, i agree with everyone on this site this m...\n",
       "5           5  jennifer ehle was sparkling in \\\"pride and pre...\n",
       "6           6  amy poehler is a terrific comedian on saturday...\n",
       "7           7  a plane carrying employees of a large biotech ...\n",
       "8           8  a well made, gritty science fiction movie, it ...\n",
       "9           9  incredibly dumb and utterly predictable story ..."
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98af72",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f04fca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 코퍼스를 단어로 토큰화한 후 'word_tokens' 라는 컬럼을 생성하기\n",
    "# word_tokenize 함수 사용\n",
    "# 1. df에서 review 컬럼 접근\n",
    "# 2. apply 메서드를 사용해서 word_tokenize 함수 사용 / .apply(함수이름)\n",
    "# 3. word_tokens 라는 컬럼을 생성하여 값 담아주기\n",
    "df['word_tokens'] = df['review'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "371bb8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review  \\\n",
       "0           0  watching time chasers, it obvious that it was ...   \n",
       "1           1  i saw this film about 20 years ago and remembe...   \n",
       "2           2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3           3  i went to see this film with a great deal of e...   \n",
       "4           4  yes, i agree with everyone on this site this m...   \n",
       "5           5  jennifer ehle was sparkling in \\\"pride and pre...   \n",
       "6           6  amy poehler is a terrific comedian on saturday...   \n",
       "7           7  a plane carrying employees of a large biotech ...   \n",
       "8           8  a well made, gritty science fiction movie, it ...   \n",
       "9           9  incredibly dumb and utterly predictable story ...   \n",
       "\n",
       "                                         word_tokens  \n",
       "0  [watching, time, chasers, ,, it, obvious, that...  \n",
       "1  [i, saw, this, film, about, 20, years, ago, an...  \n",
       "2  [minor, spoilers, in, new, york, ,, joan, barn...  \n",
       "3  [i, went, to, see, this, film, with, a, great,...  \n",
       "4  [yes, ,, i, agree, with, everyone, on, this, s...  \n",
       "5  [jennifer, ehle, was, sparkling, in, \\, '', pr...  \n",
       "6  [amy, poehler, is, a, terrific, comedian, on, ...  \n",
       "7  [a, plane, carrying, employees, of, a, large, ...  \n",
       "8  [a, well, made, ,, gritty, science, fiction, m...  \n",
       "9  [incredibly, dumb, and, utterly, predictable, ...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3004cfd",
   "metadata": {},
   "source": [
    "### 데이터 정제\n",
    "- 등장빈도, 단어길이, 불용어 세트를 사용하여 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4ca86411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# 파이썬 모듈 파일이 중간에 수정되면 해당 내용이 자동으로 반영\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e879c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 정의\n",
    "def plus(a,b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "114d0259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x, y)>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 람다 함수 사용\n",
    "lambda x, y : x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5424233f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [watching, time, chasers, ,, it, obvious, that...\n",
       "1    [i, saw, this, film, about, 20, years, ago, an...\n",
       "2    [minor, spoilers, in, new, york, ,, joan, barn...\n",
       "3    [i, went, to, see, this, film, with, a, great,...\n",
       "4    [yes, ,, i, agree, with, everyone, on, this, s...\n",
       "5    [jennifer, ehle, was, sparkling, in, \\, '', pr...\n",
       "6    [amy, poehler, is, a, terrific, comedian, on, ...\n",
       "7    [a, plane, carrying, employees, of, a, large, ...\n",
       "8    [a, well, made, ,, gritty, science, fiction, m...\n",
       "9    [incredibly, dumb, and, utterly, predictable, ...\n",
       "Name: word_tokens, dtype: object"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4d4cc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set= set(stopwords.words('english')) # 불용어 세트\n",
    "# 빈도수에 따른 데이터 정제 함수 : clean_by_freq\n",
    "# 단어길이에 따른 데이터 정제 함수 : clean_by_len\n",
    "# 불용어 세트를 활용한 정제 함수 : clean_by_stopwords\n",
    "\n",
    "# 빈도수 정제 함수 사용 -> df['cleaned_words']에 대입\n",
    "df['cleaned_words'] = df['word_tokens'].apply(lambda x:clean_by_freq(x,1)) \n",
    "## 람다함수의 x의 값에는 df['word_tokens']의 행값이 들어감!\n",
    "\n",
    "# 단어길이(2) 정제함수 적용 -> df['cleaned_words'] 재대입\n",
    "df['cleaned_words'] = df['cleaned_words'].apply(lambda x:clean_by_len(x,2))\n",
    "\n",
    "# 불용어 정제 함수 적용 -> df['cleaned_words'] 재대입\n",
    "df['cleaned_words'] = df['cleaned_words'].apply(lambda x:clean_by_stopwords(x,stopwords_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "82225fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [one, film, said, '', 's, really, bad, movie, ...\n",
       "1                                         [film, film]\n",
       "2    [new, york, joan, barnard, elvire, audrey, bar...\n",
       "3    [went, film, film, 's, went, 's, jump, send, n...\n",
       "4    [site, movie, bad, even, movie, made, 's, movi...\n",
       "5    [ehle, '', '', northam, wonderful, '', '', won...\n",
       "6    [role, movie, n't, 's, author, book, author, '...\n",
       "7    [plane, --, ceo, 's, --, search, rescue, missi...\n",
       "8    [gritty, movie, sci-fi, good, suspense, movie,...\n",
       "9         [girl, girl, '', '', --, --, '', '', '', '']\n",
       "Name: cleaned_words, dtype: object"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_words']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f8b07",
   "metadata": {},
   "source": [
    "### 어간추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "77d8574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming_by_porter 함수 적용하기\n",
    "# cleaned_words 컬럼값에 함수 적용\n",
    "# stemmed_tokens 라는 컬럼 생성\n",
    "df['stemmed_tokens'] = df['cleaned_words'].apply(stemming_by_porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cd0d9023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [one, film, said, '', 's, realli, bad, movi, '...\n",
       "1                                         [film, film]\n",
       "2    [new, york, joan, barnard, elvir, audrey, barn...\n",
       "3    [went, film, film, 's, went, 's, jump, send, n...\n",
       "4    [site, movi, bad, even, movi, made, 's, movi, ...\n",
       "5    [ehl, '', '', northam, wonder, '', '', wonder,...\n",
       "6    [role, movi, n't, 's, author, book, author, 's...\n",
       "7    [plane, --, ceo, 's, --, search, rescu, missio...\n",
       "8    [gritti, movi, sci-fi, good, suspens, movi, sc...\n",
       "9         [girl, girl, '', '', --, --, '', '', '', '']\n",
       "Name: stemmed_tokens, dtype: object"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stemmed_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7e7db624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 토큰화 진행\n",
    "# sent_tokenize 함수 적용\n",
    "# 'sent_tokens' 라는 컬럼 생성\n",
    "df['sent_tokens'] = df['review'].apply(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4cbabd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watching time chasers, it obvious that it was made by a bunch of friends.',\n",
       " 'maybe they were sitting around one day in film school and said, \\\\\"hey, let\\'s pool our money together and make a really bad movie!\\\\\" or something like that.',\n",
       " 'what ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc.',\n",
       " \"all corners were cut, except the one that would have prevented this film's release.\",\n",
       " \"life's like that.\"]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sent_tokens'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4269151",
   "metadata": {},
   "source": [
    "### 품사 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "82d72c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tagger 함수 사용하여 'sent_tokens' 컬럼에 적용\n",
    "# pos_tagged_tokens라는 컬럼 생성하기\n",
    "df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8b82c459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('watching', 'VBG'), ('time', 'NN'), ('chasers', 'NNS'), (',', ','), ('it', 'PRP'), ('obvious', 'VBZ'), ('that', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('made', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('bunch', 'NN'), ('of', 'IN'), ('friends', 'NNS'), ('.', '.'), ('maybe', 'RB'), ('they', 'PRP'), ('were', 'VBD'), ('sitting', 'VBG'), ('around', 'IN'), ('one', 'CD'), ('day', 'NN'), ('in', 'IN'), ('film', 'NN'), ('school', 'NN'), ('and', 'CC'), ('said', 'VBD'), (',', ','), ('\\\\', 'NNP'), (\"''\", \"''\"), ('hey', 'NN'), (',', ','), ('let', 'VB'), (\"'s\", 'POS'), ('pool', 'VB'), ('our', 'PRP$'), ('money', 'NN'), ('together', 'RB'), ('and', 'CC'), ('make', 'VB'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('!', '.'), ('\\\\', 'NN'), (\"''\", \"''\"), ('or', 'CC'), ('something', 'NN'), ('like', 'IN'), ('that', 'DT'), ('.', '.'), ('what', 'WP'), ('ever', 'RB'), ('they', 'PRP'), ('said', 'VBD'), (',', ','), ('they', 'PRP'), ('still', 'RB'), ('ended', 'VBD'), ('up', 'RP'), ('making', 'VBG'), ('a', 'DT'), ('really', 'RB'), ('bad', 'JJ'), ('movie', 'NN'), ('--', ':'), ('dull', 'JJ'), ('story', 'NN'), (',', ','), ('bad', 'JJ'), ('script', 'NN'), (',', ','), ('lame', 'NN'), ('acting', 'NN'), (',', ','), ('poor', 'JJ'), ('cinematography', 'NN'), (',', ','), ('bottom', 'NN'), ('of', 'IN'), ('the', 'DT'), ('barrel', 'NN'), ('stock', 'NN'), ('music', 'NN'), (',', ','), ('etc', 'FW'), ('.', '.'), ('all', 'DT'), ('corners', 'NNS'), ('were', 'VBD'), ('cut', 'VBN'), (',', ','), ('except', 'IN'), ('the', 'DT'), ('one', 'NN'), ('that', 'WDT'), ('would', 'MD'), ('have', 'VB'), ('prevented', 'VBN'), ('this', 'DT'), ('film', 'NN'), (\"'s\", 'POS'), ('release', 'NN'), ('.', '.'), ('life', 'NN'), (\"'s\", 'POS'), ('like', 'IN'), ('that', 'DT'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(df['pos_tagged_tokens'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea877c",
   "metadata": {},
   "source": [
    "### 표제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a0c10c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_lemmatizer 함수 사용하여 'pos_tagged_tokens' 컬럼에서 표제어 추출하기\n",
    "# 'lemmatized_tokens' 라는 컬럼 생성\n",
    "\n",
    "df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(word_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e461302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['watch', 'time', 'chaser', ',', 'it', 'obvious', 'that', 'it', 'be', 'make', 'by', 'a', 'bunch', 'of', 'friend', '.', 'maybe', 'they', 'be', 'sit', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'say', ',', '\\\\', \"''\", 'hey', ',', 'let', \"'s\", 'pool', 'our', 'money', 'together', 'and', 'make', 'a', 'really', 'bad', 'movie', '!', '\\\\', \"''\", 'or', 'something', 'like', 'that', '.', 'what', 'ever', 'they', 'say', ',', 'they', 'still', 'end', 'up', 'make', 'a', 'really', 'bad', 'movie', '--', 'dull', 'story', ',', 'bad', 'script', ',', 'lame', 'acting', ',', 'poor', 'cinematography', ',', 'bottom', 'of', 'the', 'barrel', 'stock', 'music', ',', 'etc', '.', 'all', 'corner', 'be', 'cut', ',', 'except', 'the', 'one', 'that', 'would', 'have', 'prevent', 'this', 'film', \"'s\", 'release', '.', 'life', \"'s\", 'like', 'that', '.']\n"
     ]
    }
   ],
   "source": [
    "print(df['lemmatized_tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "510ec213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>cleaned_words</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "      <td>[one, film, said, '', 's, really, bad, movie, ...</td>\n",
       "      <td>[one, film, said, '', 's, realli, bad, movi, '...</td>\n",
       "      <td>[watching time chasers, it obvious that it was...</td>\n",
       "      <td>[(watching, VBG), (time, NN), (chasers, NNS), ...</td>\n",
       "      <td>[watch, time, chaser, ,, it, obvious, that, it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, film]</td>\n",
       "      <td>[film, film]</td>\n",
       "      <td>[i saw this film about 20 years ago and rememb...</td>\n",
       "      <td>[(i, NN), (saw, VBD), (this, DT), (film, NN), ...</td>\n",
       "      <td>[i, saw, this, film, about, 20, year, ago, and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "      <td>[new, york, joan, barnard, elvir, audrey, barn...</td>\n",
       "      <td>[minor spoilers in new york, joan barnard (elv...</td>\n",
       "      <td>[(minor, JJ), (spoilers, NNS), (in, IN), (new,...</td>\n",
       "      <td>[minor, spoiler, in, new, york, ,, joan, barna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, film, 's, went, 's, jump, send, n...</td>\n",
       "      <td>[went, film, film, 's, went, 's, jump, send, n...</td>\n",
       "      <td>[i went to see this film with a great deal of ...</td>\n",
       "      <td>[(i, JJ), (went, VBD), (to, TO), (see, VB), (t...</td>\n",
       "      <td>[i, go, to, see, this, film, with, a, great, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, 's, movi...</td>\n",
       "      <td>[site, movi, bad, even, movi, made, 's, movi, ...</td>\n",
       "      <td>[yes, i agree with everyone on this site this ...</td>\n",
       "      <td>[(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "      <td>[ehle, '', '', northam, wonderful, '', '', won...</td>\n",
       "      <td>[ehl, '', '', northam, wonder, '', '', wonder,...</td>\n",
       "      <td>[jennifer ehle was sparkling in \\\"pride and pr...</td>\n",
       "      <td>[(jennifer, NN), (ehle, NN), (was, VBD), (spar...</td>\n",
       "      <td>[jennifer, ehle, be, sparkle, in, \\, '', pride...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, 's, author, book, author, '...</td>\n",
       "      <td>[role, movi, n't, 's, author, book, author, 's...</td>\n",
       "      <td>[amy poehler is a terrific comedian on saturda...</td>\n",
       "      <td>[(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...</td>\n",
       "      <td>[amy, poehler, be, a, terrific, comedian, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "      <td>[plane, --, ceo, 's, --, search, rescue, missi...</td>\n",
       "      <td>[plane, --, ceo, 's, --, search, rescu, missio...</td>\n",
       "      <td>[a plane carrying employees of a large biotech...</td>\n",
       "      <td>[(a, DT), (plane, NN), (carrying, VBG), (emplo...</td>\n",
       "      <td>[a, plane, carry, employee, of, a, large, biot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "      <td>[gritti, movi, sci-fi, good, suspens, movi, sc...</td>\n",
       "      <td>[a well made, gritty science fiction movie, it...</td>\n",
       "      <td>[(a, DT), (well, NN), (made, VBN), (,, ,), (gr...</td>\n",
       "      <td>[a, well, make, ,, gritty, science, fiction, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl, '', '', --, --, '', '', '', '']</td>\n",
       "      <td>[girl, girl, '', '', --, --, '', '', '', '']</td>\n",
       "      <td>[incredibly dumb and utterly predictable story...</td>\n",
       "      <td>[(incredibly, RB), (dumb, JJ), (and, CC), (utt...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review  \\\n",
       "0           0  watching time chasers, it obvious that it was ...   \n",
       "1           1  i saw this film about 20 years ago and remembe...   \n",
       "2           2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3           3  i went to see this film with a great deal of e...   \n",
       "4           4  yes, i agree with everyone on this site this m...   \n",
       "5           5  jennifer ehle was sparkling in \\\"pride and pre...   \n",
       "6           6  amy poehler is a terrific comedian on saturday...   \n",
       "7           7  a plane carrying employees of a large biotech ...   \n",
       "8           8  a well made, gritty science fiction movie, it ...   \n",
       "9           9  incredibly dumb and utterly predictable story ...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [watching, time, chasers, ,, it, obvious, that...   \n",
       "1  [i, saw, this, film, about, 20, years, ago, an...   \n",
       "2  [minor, spoilers, in, new, york, ,, joan, barn...   \n",
       "3  [i, went, to, see, this, film, with, a, great,...   \n",
       "4  [yes, ,, i, agree, with, everyone, on, this, s...   \n",
       "5  [jennifer, ehle, was, sparkling, in, \\, '', pr...   \n",
       "6  [amy, poehler, is, a, terrific, comedian, on, ...   \n",
       "7  [a, plane, carrying, employees, of, a, large, ...   \n",
       "8  [a, well, made, ,, gritty, science, fiction, m...   \n",
       "9  [incredibly, dumb, and, utterly, predictable, ...   \n",
       "\n",
       "                                       cleaned_words  \\\n",
       "0  [one, film, said, '', 's, really, bad, movie, ...   \n",
       "1                                       [film, film]   \n",
       "2  [new, york, joan, barnard, elvire, audrey, bar...   \n",
       "3  [went, film, film, 's, went, 's, jump, send, n...   \n",
       "4  [site, movie, bad, even, movie, made, 's, movi...   \n",
       "5  [ehle, '', '', northam, wonderful, '', '', won...   \n",
       "6  [role, movie, n't, 's, author, book, author, '...   \n",
       "7  [plane, --, ceo, 's, --, search, rescue, missi...   \n",
       "8  [gritty, movie, sci-fi, good, suspense, movie,...   \n",
       "9       [girl, girl, '', '', --, --, '', '', '', '']   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [one, film, said, '', 's, realli, bad, movi, '...   \n",
       "1                                       [film, film]   \n",
       "2  [new, york, joan, barnard, elvir, audrey, barn...   \n",
       "3  [went, film, film, 's, went, 's, jump, send, n...   \n",
       "4  [site, movi, bad, even, movi, made, 's, movi, ...   \n",
       "5  [ehl, '', '', northam, wonder, '', '', wonder,...   \n",
       "6  [role, movi, n't, 's, author, book, author, 's...   \n",
       "7  [plane, --, ceo, 's, --, search, rescu, missio...   \n",
       "8  [gritti, movi, sci-fi, good, suspens, movi, sc...   \n",
       "9       [girl, girl, '', '', --, --, '', '', '', '']   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [watching time chasers, it obvious that it was...   \n",
       "1  [i saw this film about 20 years ago and rememb...   \n",
       "2  [minor spoilers in new york, joan barnard (elv...   \n",
       "3  [i went to see this film with a great deal of ...   \n",
       "4  [yes, i agree with everyone on this site this ...   \n",
       "5  [jennifer ehle was sparkling in \\\"pride and pr...   \n",
       "6  [amy poehler is a terrific comedian on saturda...   \n",
       "7  [a plane carrying employees of a large biotech...   \n",
       "8  [a well made, gritty science fiction movie, it...   \n",
       "9  [incredibly dumb and utterly predictable story...   \n",
       "\n",
       "                                   pos_tagged_tokens  \\\n",
       "0  [(watching, VBG), (time, NN), (chasers, NNS), ...   \n",
       "1  [(i, NN), (saw, VBD), (this, DT), (film, NN), ...   \n",
       "2  [(minor, JJ), (spoilers, NNS), (in, IN), (new,...   \n",
       "3  [(i, JJ), (went, VBD), (to, TO), (see, VB), (t...   \n",
       "4  [(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...   \n",
       "5  [(jennifer, NN), (ehle, NN), (was, VBD), (spar...   \n",
       "6  [(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...   \n",
       "7  [(a, DT), (plane, NN), (carrying, VBG), (emplo...   \n",
       "8  [(a, DT), (well, NN), (made, VBN), (,, ,), (gr...   \n",
       "9  [(incredibly, RB), (dumb, JJ), (and, CC), (utt...   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0  [watch, time, chaser, ,, it, obvious, that, it...  \n",
       "1  [i, saw, this, film, about, 20, year, ago, and...  \n",
       "2  [minor, spoiler, in, new, york, ,, joan, barna...  \n",
       "3  [i, go, to, see, this, film, with, a, great, d...  \n",
       "4  [yes, ,, i, agree, with, everyone, on, this, s...  \n",
       "5  [jennifer, ehle, be, sparkle, in, \\, '', pride...  \n",
       "6  [amy, poehler, be, a, terrific, comedian, on, ...  \n",
       "7  [a, plane, carry, employee, of, a, large, biot...  \n",
       "8  [a, well, make, ,, gritty, science, fiction, m...  \n",
       "9  [incredibly, dumb, and, utterly, predictable, ...  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "6593b831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [make, one, film, say, '', 's, make, really, b...\n",
       "1                                         [film, film]\n",
       "2    [new, york, joan, barnard, elvire, audrey, bar...\n",
       "3    [go, film, film, 's, go, 's, jump, send, n't, ...\n",
       "4    [site, movie, bad, even, movie, movie, make, '...\n",
       "5    [ehle, '', '', northam, wonderful, '', '', won...\n",
       "6    [role, movie, n't, 's, author, book, funny, au...\n",
       "7    [plane, --, ceo, 's, --, go, search, rescue, m...\n",
       "8    [gritty, movie, movie, keep, sci-fi, good, kee...\n",
       "9         [girl, girl, '', '', --, --, '', '', '', '']\n",
       "Name: cleaned_tokens, dtype: object"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추가 전처리\n",
    "# 불용어 세트 = stopwords_set\n",
    "stopwords_set= set(stopwords.words('english')) \n",
    "\n",
    "# lemmatized_tokens 에 빈도수(clean_by_freq)=1, 단어길이(clean_by_len)=2, 불용어 정제(clean_by_stopwords),\n",
    "# 불용어 정제 (clean_by_stopwords) 함수 사용\n",
    "# cleaned_tokens라는 컬럼 생성\n",
    "\n",
    "df['cleaned_tokens'] = df['lemmatized_tokens'].apply(lambda x:clean_by_freq(x,1)) \n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x:clean_by_len(x,2))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x:clean_by_stopwords(x,stopwords_set))\n",
    "df['cleaned_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d2a9e",
   "metadata": {},
   "source": [
    "### 정수 인코딩\n",
    "- 텍스트 데이터를 숫자 데이터로 변환\n",
    "- 등장빈도를 기준으로 정렬한 다음 인덱스 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b2d26",
   "metadata": {},
   "source": [
    "#### 1) 하나의 로우(행) 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f1fddc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['site', 'movie', 'bad', 'even', 'movie', 'movie', 'make', \"'s\", 'movie', 'special', 'describe', 'movie', 'movie', 'describe', 'movie', 'jim', 'make', 'stand-up', 'day', 'stand-up', 'jim', 'like', 'jim', 'actor', 'love', 'stand', 'day', 'comedian', 'special', 'jim', 'day', 'even', 'site', 'love', 'jim', 'stand-up', 'jim', 'actor', 'movie', 'stand', 'comedian', 'jim', 'like', \"''\", \"'s\", \"''\", \"''\", 'really', \"''\", 'terrible', 'really', 'terrible', 'movie', 'terrible', 'really', 'bad', 'movie']\n"
     ]
    }
   ],
   "source": [
    "tokens = df['cleaned_tokens'][4]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d2b209c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 10), ('jim', 7), (\"''\", 4), ('stand-up', 3), ('day', 3), ('really', 3), ('terrible', 3), ('site', 2), ('bad', 2), ('even', 2), ('make', 2), (\"'s\", 2), ('special', 2), ('describe', 2), ('like', 2), ('actor', 2), ('love', 2), ('stand', 2), ('comedian', 2)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter(tokens)\n",
    "# 단어 토큰들의 빈도수가 높은 순서대로 정렬 - most_common()\n",
    "vocab = vocab.most_common()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e7732397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어에 인덱스 부여\n",
    "word_to_idx ={}\n",
    "i=0\n",
    "for (word,freq) in vocab:\n",
    "    i+=1\n",
    "    word_to_idx[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a54bc666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie': 1, 'jim': 2, \"''\": 3, 'stand-up': 4, 'day': 5, 'really': 6, 'terrible': 7, 'site': 8, 'bad': 9, 'even': 10, 'make': 11, \"'s\": 12, 'special': 13, 'describe': 14, 'like': 15, 'actor': 16, 'love': 17, 'stand': 18, 'comedian': 19}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4a4dd803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['site', 'movie', 'bad', 'even', 'movie', 'movie', 'make', \"'s\", 'movie', 'special', 'describe', 'movie', 'movie', 'describe', 'movie', 'jim', 'make', 'stand-up', 'day', 'stand-up', 'jim', 'like', 'jim', 'actor', 'love', 'stand', 'day', 'comedian', 'special', 'jim', 'day', 'even', 'site', 'love', 'jim', 'stand-up', 'jim', 'actor', 'movie', 'stand', 'comedian', 'jim', 'like', \"''\", \"'s\", \"''\", \"''\", 'really', \"''\", 'terrible', 'really', 'terrible', 'movie', 'terrible', 'really', 'bad', 'movie']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b8193dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 1, 9, 10, 1, 1, 11, 12, 1, 13, 14, 1, 1, 14, 1, 2, 11, 4, 5, 4, 2, 15, 2, 16, 17, 18, 5, 19, 13, 2, 5, 10, 8, 17, 2, 4, 2, 16, 1, 18, 19, 2, 15, 3, 12, 3, 3, 6, 3, 7, 6, 7, 1, 7, 6, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "encoded_idx = []\n",
    "for token in tokens:\n",
    "    idx = word_to_idx[token]\n",
    "    encoded_idx.append(idx)\n",
    "print(encoded_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d697ddd",
   "metadata": {},
   "source": [
    "#### 2) 전체 데이터 프레임 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b52f06bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'one', 'film', 'say', \"''\", \"'s\", 'make', 'really', 'bad', 'movie', \"''\", 'like', 'say', 'make', 'really', 'bad', 'movie', 'bad', 'one', 'film', \"'s\", \"'s\", 'like', 'film', 'film', 'new', 'york', 'joan', 'barnard', 'elvire', 'audrey', 'barnard', 'john', 'saxon', 'italy', 'etruscan', 'tomb', 'joan', 'italy', 'colleague', 'italy', 'maggot', 'maggot', 'joan', 'drug', 'drug', 'tomb', 'colleague', 'story', 'end', 'new', 'york', 'joan', 'colleague', 'romantic', 'end', 'waste', 'time', 'watch', 'story', 'romantic', 'end', 'elvire', 'audrey', 'john', 'saxon', 'maggot', 'watch', 'etrusco', 'go', 'watch', 'go', 'go', 'waste', 'time', 'etrusco', 'etruscan', 'go', 'film', 'film', \"'s\", 'go', \"'s\", 'jump', 'send', \"n't\", 'jump', 'radio', \"n't\", 'send', 'reporter', 'fear', 'jump', 'fear', 'radio', 'reporter', \"n't\", 'radio', \"n't\", 'go', \"n't\", 'site', 'movie', 'bad', 'even', 'movie', 'movie', 'make', \"'s\", 'movie', 'special', 'describe', 'movie', 'movie', 'describe', 'movie', 'jim', 'make', 'stand-up', 'day', 'stand-up', 'jim', 'like', 'jim', 'actor', 'love', 'stand', 'day', 'comedian', 'special', 'jim', 'day', 'even', 'site', 'love', 'jim', 'stand-up', 'jim', 'actor', 'movie', 'stand', 'comedian', 'jim', 'like', \"''\", \"'s\", \"''\", \"''\", 'really', \"''\", 'terrible', 'really', 'terrible', 'movie', 'terrible', 'really', 'bad', 'movie', 'ehle', \"''\", \"''\", 'northam', 'wonderful', \"''\", \"''\", 'wonderful', 'ehle', 'northam', 'lust', 'lust', 'ehle', 'northam', 'role', 'movie', \"n't\", \"'s\", 'author', 'book', 'funny', 'author', \"'s\", 'author', 'role', \"n't\", 'funny', 'queen', 'corn', 'corn', 'queen', 'author', 'book', 'movie', \"n't\", 'plane', '--', 'ceo', \"'s\", '--', 'go', 'search', 'rescue', 'mission', 'call', 'ceo', 'harlan', 'knowles', 'lance', 'henriksen', 'put', 'search', 'rescue', 'mission', 'knowles', 'search', 'try', 'rescue', 'wood', 'film', 'one', 'lance', 'henriksen', 'one', 'two', 'could', 'easily', 'decent', 'film', 'two', \"'re\", 'quastel', \"'s\", '--', 'film', 'call', 'sasquatch', 'bad', 'edit', 'see', 'quastel', \"'s\", 'appear', \"''\", \"'s\", 'try', 'time', 'want', 'try', \"''\", 'potential', 'material', 'relate', 'plane', 'try', 'crib', 'material', 'relate', 'monster', 'crib', 'exposition', 'dialogue', 'potential', 'far', 'monster', 'costume', 'get', 'see', 'character', 'wood', 'could', 'quastel', 'would', 'stereotype', 'time', 'monster', \"'s\", \"''\", \"''\", 'edit', 'well', 'scene', 'decent', 'dialogue', 'could', 'easily', 'effective', 'sasquatch', 'make', 'reason', 'quastel', 'think', \"'s\", 'good', 'idea', 'dialogue', 'scene', 'occur', 'time', 'see', 'line', 'scene', 'line', 'scene', 'back', 'back', 'reason', 'think', \"'s\", 'good', 'idea', 'use', 'use', 'dialogue', 'whether', 'need', 'idea', 'time', 'irrelevant', 'comment', 'whether', 'irrelevant', 'comment', 'occur', 'one', 'time', 'reason', \"n't\", 'whether', 'scene', 'cut', 'random', 'scene', \"'re\", 'show', 'appear', 'random', 'important', 'either', 'never', 'appear', \"'re\", 'far', 'scene', 'reason', 'leave', 'scene', 'film', 'either', 'need', 'exposition', 'get', 'need', 'cut', \"'s\", 'important', 'monster', \"'s\", \"''\", \"''\", 'could', 'easily', 'show', 'reason', 'character', '--', '--', 'leave', 'even', 'though', \"'s\", 'reason', 'go', 'scene', 'even', 'though', 'never', 'reason', 'character', 'call', 'harlan', 'knowles', \"''\", \"''\", 'like', \"'re\", 'stereotype', 'reason', 'quastel', 'use', \"''\", \"''\", \"''\", \"''\", 'monster', 'scene', 'even', 'though', 'costume', \"n't\", 'bad', 'would', 'effective', 'put', 'bad', 'could', 'go', 'get', 'idea', 'want', 'like', 'film', 'good', \"'m\", 'henriksen', \"'m\", 'love', 'love', 'film', 'one', '--', 'could', \"n't\", 'time', 'think', \"''\", 'go', 'well', \"''\", 'quastel', 'make', 'gritty', 'movie', 'movie', 'keep', 'sci-fi', 'good', 'keep', 'suspense', 'look', 'movie', 'sci-fi', \"'re\", 'look', \"'re\", 'look', 'good', 'gritty', 'sci-fi', 'good', 'suspense', 'movie', 'good', 'girl', 'girl', \"''\", \"''\", '--', '--', \"''\", \"''\", \"''\", \"''\"]\n"
     ]
    }
   ],
   "source": [
    "# 전체 코퍼스의 토큰들을 전부 합하여 단어의 등장 빈도 계산\n",
    "print(sum(df['cleaned_tokens'],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3f573846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"''\": 1, \"'s\": 2, 'movie': 3, 'film': 4, \"n't\": 5, 'go': 6, 'scene': 7, 'bad': 8, 'time': 9, '--': 10, 'reason': 11, 'make': 12, 'jim': 13, 'good': 14, 'one': 15, 'like': 16, 'could': 17, \"'re\": 18, 'quastel': 19, 'really': 20, 'even': 21, 'monster': 22, 'joan': 23, 'love': 24, 'author': 25, 'try': 26, 'dialogue': 27, 'idea': 28, 'italy': 29, 'colleague': 30, 'maggot': 31, 'end': 32, 'watch': 33, 'jump': 34, 'radio': 35, 'stand-up': 36, 'day': 37, 'terrible': 38, 'ehle': 39, 'northam': 40, 'search': 41, 'rescue': 42, 'call': 43, 'knowles': 44, 'henriksen': 45, 'easily': 46, 'see': 47, 'appear': 48, 'get': 49, 'character': 50, 'think': 51, 'use': 52, 'whether': 53, 'need': 54, 'though': 55, 'sci-fi': 56, 'look': 57, 'say': 58, 'new': 59, 'york': 60, 'barnard': 61, 'elvire': 62, 'audrey': 63, 'john': 64, 'saxon': 65, 'etruscan': 66, 'tomb': 67, 'drug': 68, 'story': 69, 'romantic': 70, 'waste': 71, 'etrusco': 72, 'send': 73, 'reporter': 74, 'fear': 75, 'site': 76, 'special': 77, 'describe': 78, 'actor': 79, 'stand': 80, 'comedian': 81, 'wonderful': 82, 'lust': 83, 'role': 84, 'book': 85, 'funny': 86, 'queen': 87, 'corn': 88, 'plane': 89, 'ceo': 90, 'mission': 91, 'harlan': 92, 'lance': 93, 'put': 94, 'wood': 95, 'two': 96, 'decent': 97, 'sasquatch': 98, 'edit': 99, 'want': 100, 'potential': 101, 'material': 102, 'relate': 103, 'crib': 104, 'exposition': 105, 'far': 106, 'costume': 107, 'would': 108, 'stereotype': 109, 'well': 110, 'effective': 111, 'occur': 112, 'line': 113, 'back': 114, 'irrelevant': 115, 'comment': 116, 'cut': 117, 'random': 118, 'show': 119, 'important': 120, 'either': 121, 'never': 122, 'leave': 123, \"'m\": 124, 'gritty': 125, 'keep': 126, 'suspense': 127, 'girl': 128}\n"
     ]
    }
   ],
   "source": [
    "# 전체 단어에 인덱스 부여\n",
    "word_to_idx = {}\n",
    "i = 0\n",
    "tokens = sum(df['cleaned_tokens'],[])\n",
    "# 등장빈도 계산 후 vocab 변수에 담아주기 - Counter()\n",
    "vocab = Counter(tokens)\n",
    "# 등장 빈도 순서대로 정렬하기 - most_common()\n",
    "vocab = vocab.most_common()\n",
    "\n",
    "for (word,freq) in vocab:\n",
    "    i+=1\n",
    "    word_to_idx[word] = i\n",
    "print(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "744ecdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩 함수 정의\n",
    "def idx_encoder(tokens,word_to_idx):\n",
    "    encoded_idx = [] # 인코딩한 값을 담아줄 리스트\n",
    "    \n",
    "    for token in tokens:\n",
    "        idx = word_to_idx[token]\n",
    "        encoded_idx.append(idx)\n",
    "        \n",
    "    return encoded_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "00e36390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [12, 15, 4, 58, 1, 2, 12, 20, 8, 3, 1, 16, 58,...\n",
       "1                                               [4, 4]\n",
       "2    [59, 60, 23, 61, 62, 63, 61, 64, 65, 29, 66, 6...\n",
       "3    [6, 4, 4, 2, 6, 2, 34, 73, 5, 34, 35, 5, 73, 7...\n",
       "4    [76, 3, 8, 21, 3, 3, 12, 2, 3, 77, 78, 3, 3, 7...\n",
       "5    [39, 1, 1, 40, 82, 1, 1, 82, 39, 40, 83, 83, 3...\n",
       "6    [84, 3, 5, 2, 25, 85, 86, 25, 2, 25, 84, 5, 86...\n",
       "7    [89, 10, 90, 2, 10, 6, 41, 42, 91, 43, 90, 92,...\n",
       "8    [125, 3, 3, 126, 56, 14, 126, 127, 57, 3, 56, ...\n",
       "9                 [128, 128, 1, 1, 10, 10, 1, 1, 1, 1]\n",
       "Name: cleaned_tokens, dtype: object"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_tokens'].apply(lambda x:idx_encoder(x,word_to_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf43734",
   "metadata": {},
   "source": [
    "### 감정분석\n",
    "- 자연어에 담긴 어조가 긍정적인지, 부정적인지 혹은 중립적인지 확인하는 작업\n",
    "- VADER 감성분석기 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba87bc",
   "metadata": {},
   "source": [
    "#### VADER 감성분석\n",
    "- 감성분석을 위한 어휘 사전이자 알고리즘\n",
    "- 축약형과 기호 등을 고려하여 감성 지수를 추출\n",
    "- 소셜미디어 텍스트를 분석할 때 자주 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "75d7b9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "                                              0.0/126.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 126.0/126.0 kB 7.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2023.5.7)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0b8959da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 로딩\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "senti_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "569fa51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.406, 'pos': 0.594, 'compound': 0.6588}\n",
      "{'neg': 0.531, 'neu': 0.469, 'pos': 0.0, 'compound': -0.5255}\n",
      "{'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.2263}\n"
     ]
    }
   ],
   "source": [
    "text1 = \"This is a great movie!\"\n",
    "text2 = \"This is a terrible movie!\"\n",
    "text3 = \"This movie was just okay.\"\n",
    "\n",
    "# VADER 감성분석\n",
    "# 단어, 문장, 여러 문장으로 이루어진 코퍼스도 바로 감성 지수 계산 가능\n",
    "# polarity_scores 메서드 사용\n",
    "# neg : 부정 감성지수\n",
    "# neu : 중립 감성지수\n",
    "# pos : 긍정 감성지수\n",
    "# compound : 위 3개 지수를 적절하게 조합하여 -1(부정)과 1(긍정) 사이의 감성지수를 나타냄\n",
    "print(senti_analyzer.polarity_scores(text1))\n",
    "print(senti_analyzer.polarity_scores(text2))\n",
    "print(senti_analyzer.polarity_scores(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e4ee428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  watching time chasers, it obvious that it was ...\n",
       "1  i saw this film about 20 years ago and remembe...\n",
       "2  minor spoilers in new york, joan barnard (elvi...\n",
       "3  i went to see this film with a great deal of e...\n",
       "4  yes, i agree with everyone on this site this m...\n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...\n",
       "6  amy poehler is a terrific comedian on saturday...\n",
       "7  a plane carrying employees of a large biotech ...\n",
       "8  a well made, gritty science fiction movie, it ...\n",
       "9  incredibly dumb and utterly predictable story ..."
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5075f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER 감성분석 함수 정의\n",
    "def vader_sentiment(text):\n",
    "    senti_analyzer = SentimentIntensityAnalyzer()\n",
    "    # 감성분석 - polarity_scores 메서드\n",
    "    senti_score = senti_analyzer.polarity_scores(text)['compound']\n",
    "    return senti_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7ad0532d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>varder_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>-0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>-0.9694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>-0.2794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>-0.9707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>0.8444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>0.9494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>0.8473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>0.9885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>0.9887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>-0.7375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  varder_sentiment\n",
       "0  watching time chasers, it obvious that it was ...           -0.9095\n",
       "1  i saw this film about 20 years ago and remembe...           -0.9694\n",
       "2  minor spoilers in new york, joan barnard (elvi...           -0.2794\n",
       "3  i went to see this film with a great deal of e...           -0.9707\n",
       "4  yes, i agree with everyone on this site this m...            0.8444\n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...            0.9494\n",
       "6  amy poehler is a terrific comedian on saturday...            0.8473\n",
       "7  a plane carrying employees of a large biotech ...            0.9885\n",
       "8  a well made, gritty science fiction movie, it ...            0.9887\n",
       "9  incredibly dumb and utterly predictable story ...           -0.7375"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['varder_sentiment'] = df['review'].apply(vader_sentiment)\n",
    "df[['review','varder_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "7ef31666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3   -0.9707\n",
      "1   -0.9694\n",
      "0   -0.9095\n",
      "9   -0.7375\n",
      "2   -0.2794\n",
      "4    0.8444\n",
      "6    0.8473\n",
      "5    0.9494\n",
      "7    0.9885\n",
      "8    0.9887\n",
      "Name: varder_sentiment, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"i went to see this film with a great deal of excitement as i was at school with the director, he was even a good friend of mine for a while. but sorry mate, this film stinks. i can only talk about what was wrong with the first half because that's when i walked out and went to the pub for a much needed drink: 1) someone's standing on a balcony about to jump and so you send a helicopter to shine a searchlight on them??? i don't think so - nothing would make them more likely to jump. 2) local radio doesn't send reporters to cover people about to attempt suicide - again for fear of pressuring them into jumping - or for fear of encouraging copy-cat instances. 3) whatever the circumstances, radio reporters don't do live broadcasts from the 10th floor of a tower block. radio cars don't carry leads long enough to connect the microphone and headphones to the transmitter. 4) the stuck in the lift scene was utterly derivative 5) the acting and direction was almost non existent.i could go on, but i won't.\""
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 감성지수가 가장 낮은 리뷰는?\n",
    "print(df['varder_sentiment'].sort_values())\n",
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "49374783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7518"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 영화의 감성지수 총합은?\n",
    "df['varder_sentiment'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5cf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48063e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1cd8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d4086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
